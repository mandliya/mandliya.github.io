{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5191e51",
   "metadata": {},
   "source": [
    "\n",
    "# From Naïve Attention to FlashAttention (in Pure PyTorch)\n",
    "### Building the algorithm from first principles, one improvement at a time\n",
    "\n",
    "This notebook is written in **blog-post style**. We’ll start from the standard “textbook” implementation of scaled dot-product attention, and then progressively refine it.\n",
    "\n",
    "The journey is the point: we begin with the simplest possible code, understand why it becomes expensive, fix numerical stability, and then redesign the computation so we no longer need to materialize the `T×T` attention matrix. That final step is the core idea behind **FlashAttention**: do the same math, but stream it in blocks, keeping memory usage small and computation stable.\n",
    "\n",
    "A follow-up post can take the final PyTorch version and translate it into a CUDA kernel. Here, we stay entirely in PyTorch so we can focus on clarity, correctness, and intuition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61bc9dd",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Setup\n",
    "\n",
    "We’ll use PyTorch and run on GPU if available. Even on CPU, you can follow the logic and verify correctness, though the benchmarks will be less dramatic.\n",
    "\n",
    "Throughout, we use these shapes:\n",
    "\n",
    "- `Q, K, V`: `[B, H, T, D]`\n",
    "  - `B`: batch size\n",
    "  - `H`: number of attention heads\n",
    "  - `T`: sequence length\n",
    "  - `D`: head dimension\n",
    "\n",
    "We’ll implement attention per head (but keep the head dimension in the tensor so the code is realistic).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = get_device()\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f9247",
   "metadata": {},
   "source": [
    "\n",
    "## 1. What attention computes\n",
    "\n",
    "Scaled dot-product attention is usually written as:\n",
    "\n",
    "\\begin{aligned}\n",
    "S &= \\frac{QK^\\top}{\\sqrt{D}} \\\\\n",
    "A &= \\mathrm{softmax}(S) \\\\\n",
    "O &= AV\n",
    "\\end{aligned}\n",
    "\n",
    "You can read this as a very specific kind of “content-based lookup”. For each query vector `q_i` (a row of `Q`), you compare it to every key vector `k_j` (rows of `K`) by taking dot products. Those dot products form the scores `S[i, j]`.\n",
    "\n",
    "The softmax turns those scores into a probability distribution over positions `j`. Finally, `O[i]` becomes a weighted average of the value vectors `v_j`.\n",
    "\n",
    "This is straightforward. The issue is that `S` has shape `[T, T]` per head, and it’s expensive to create when `T` gets large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c09f00",
   "metadata": {},
   "source": [
    "\n",
    "### A quick note on the scale factor\n",
    "\n",
    "Why divide by `sqrt(D)`? If each component of `q` and `k` has roughly unit variance, then the dot product has variance proportional to `D`. As `D` grows, the raw dot products become larger in magnitude, and softmax becomes more peaked and unstable. The scale keeps scores in a friendlier range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a7950",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Version A: naïve attention (clear but memory hungry)\n",
    "\n",
    "Let’s implement the most direct version. This matches the math exactly. It also allocates the full score matrix `S` of shape `[B, H, T, T]`.\n",
    "\n",
    "When `T` is small, this is fine. When `T` is large, this becomes the bottleneck. The core reason FlashAttention exists is that you rarely want to explicitly store `S`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attention_naive(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, causal: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Naïve scaled dot-product attention.\n",
    "    Allocates [B,H,T,T] score matrix.\n",
    "\n",
    "    Q, K, V: [B, H, T, D]\n",
    "    Returns: [B, H, T, D]\n",
    "    \"\"\"\n",
    "    assert Q.ndim == 4 and K.ndim == 4 and V.ndim == 4\n",
    "    B, H, T, D = Q.shape\n",
    "    assert K.shape == (B, H, T, D)\n",
    "    assert V.shape == (B, H, T, D)\n",
    "\n",
    "    scale = 1.0 / math.sqrt(D)\n",
    "\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale  # [B,H,T,T]\n",
    "\n",
    "    if causal:\n",
    "        mask = torch.triu(torch.ones(T, T, device=scores.device, dtype=torch.bool), diagonal=1)\n",
    "        scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "    attn = torch.softmax(scores, dim=-1)                    # [B,H,T,T]\n",
    "    out = torch.matmul(attn, V)                             # [B,H,T,D]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964c3d2",
   "metadata": {},
   "source": [
    "\n",
    "## 3. The first practical problem: numerical stability\n",
    "\n",
    "Even if we ignore memory for a moment, softmax itself can be numerically tricky.\n",
    "\n",
    "Softmax is:\n",
    "\n",
    "\\[\n",
    "\\mathrm{softmax}(s)_j = \\frac{\\exp(s_j)}{\\sum_k \\exp(s_k)}\n",
    "\\]\n",
    "\n",
    "If any `s_j` is large, `exp(s_j)` can overflow in finite precision. In practice, implementations compute:\n",
    "\n",
    "\\[\n",
    "\\mathrm{softmax}(s)_j = \\frac{\\exp(s_j - m)}{\\sum_k \\exp(s_k - m)}, \\quad m = \\max_k s_k\n",
    "\\]\n",
    "\n",
    "Subtracting `m` does not change the result, but it prevents overflow.\n",
    "\n",
    "PyTorch’s `torch.softmax` already does this internally, but it’s worth calling out because the stable trick becomes essential when we start streaming in blocks and computing softmax ourselves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1f04a",
   "metadata": {},
   "source": [
    "\n",
    "## 4. The second practical problem: the `T×T` matrix\n",
    "\n",
    "For a single head, the score matrix contains `T×T` elements. For `T=4096`, that is about 16.7 million entries.\n",
    "If you store them in fp16 (2 bytes), that’s around 33 MB per head just for the scores.\n",
    "\n",
    "The key observation behind FlashAttention is:\n",
    "\n",
    "To compute `O = softmax(S)V`, you do not need to store all of `S` at once.\n",
    "\n",
    "You can compute it in tiles: load a block of keys/values, compute a block of scores, update a running output accumulator, and then move on to the next block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf6870",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Version B: tiled computation (but still not enough)\n",
    "\n",
    "A natural first attempt is: compute scores in tiles and softmax each tile separately.\n",
    "\n",
    "Unfortunately, that is not equivalent to the full softmax. Softmax normalizes across *all* keys. If you softmax block-by-block, each block is normalized independently, which changes the result.\n",
    "\n",
    "So tiling alone is not sufficient. We need tiling plus a way to compute the global softmax normalization incrementally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6957d",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Online softmax: making tiling exact\n",
    "\n",
    "This is the heart of the story.\n",
    "\n",
    "Consider one query row `i`. Let its scores against all keys be:\n",
    "\n",
    "\\[\n",
    "s = [s_1, s_2, \\dots, s_T]\n",
    "\\]\n",
    "\n",
    "If we processed all keys at once, we would compute a stable softmax using:\n",
    "\n",
    "- `m = max(s)`\n",
    "- `l = sum(exp(s - m))`\n",
    "\n",
    "Now imagine we process keys in blocks. We want to update the same quantities as we discover more scores.\n",
    "\n",
    "Suppose we have processed some keys already, and we have a running max `m_old`, a running normalizer `l_old`, and a running weighted sum `acc_old` (which will become the output numerator).\n",
    "\n",
    "Now we receive a new block of scores `s_blk` and values `V_blk`.\n",
    "\n",
    "Let:\n",
    "- `m_blk = max(s_blk)`\n",
    "- `m_new = max(m_old, m_blk)`\n",
    "\n",
    "When the max changes, the scale of the exp terms changes. The trick is to rescale the old accumulators into the new coordinate system defined by `m_new`.\n",
    "\n",
    "The updated normalizer becomes:\n",
    "\n",
    "\\[\n",
    "l_{new} = e^{m_{old} - m_{new}} l_{old} + \\sum \\exp(s_{blk} - m_{new})\n",
    "\\]\n",
    "\n",
    "The updated weighted sum becomes:\n",
    "\n",
    "\\[\n",
    "acc_{new} = e^{m_{old} - m_{new}} acc_{old} + \\exp(s_{blk} - m_{new}) V_{blk}\n",
    "\\]\n",
    "\n",
    "At the end, the output row is simply `acc / l`. This gives an *exact* softmax over all keys, computed incrementally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7454d",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Version C: streaming attention with online softmax (FlashAttention core)\n",
    "\n",
    "Now we implement the algorithm above.\n",
    "\n",
    "We will tile across queries in blocks of `block_q` and keys/values in blocks of `block_k`.\n",
    "\n",
    "For each query block, we maintain `(m, l, acc)` for every row in that block.\n",
    "\n",
    "A detail that matters in practice is precision. Even if `Q, K, V` are fp16 or bf16, you typically compute the softmax and accumulators in fp32 to keep error under control. We’ll do the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attention_streaming_online_softmax(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    causal: bool = False,\n",
    "    block_q: int = 128,\n",
    "    block_k: int = 128,\n",
    "    softmax_dtype: torch.dtype = torch.float32,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Streaming attention using tiled score computation + online softmax.\n",
    "    This avoids allocating [B,H,T,T] and is mathematically equivalent to full softmax attention.\n",
    "\n",
    "    Q, K, V: [B, H, T, D]\n",
    "    Returns: [B, H, T, D]\n",
    "    \"\"\"\n",
    "    assert Q.ndim == 4 and K.ndim == 4 and V.ndim == 4\n",
    "    B, H, T, D = Q.shape\n",
    "    assert K.shape == (B, H, T, D)\n",
    "    assert V.shape == (B, H, T, D)\n",
    "\n",
    "    scale = 1.0 / math.sqrt(D)\n",
    "    out = torch.empty_like(V)\n",
    "\n",
    "    Qc = Q.to(dtype=softmax_dtype)\n",
    "    Kc = K.to(dtype=softmax_dtype)\n",
    "    Vc = V.to(dtype=softmax_dtype)\n",
    "\n",
    "    all_q_idx = torch.arange(T, device=Q.device)\n",
    "    all_k_idx = torch.arange(T, device=Q.device)\n",
    "\n",
    "    for q0 in range(0, T, block_q):\n",
    "        q1 = min(q0 + block_q, T)\n",
    "        q_len = q1 - q0\n",
    "\n",
    "        Qblk = Qc[:, :, q0:q1, :]  # [B,H,q_len,D]\n",
    "\n",
    "        m = torch.full((B, H, q_len), float(\"-inf\"), device=Q.device, dtype=softmax_dtype)\n",
    "        l = torch.zeros((B, H, q_len), device=Q.device, dtype=softmax_dtype)\n",
    "        acc = torch.zeros((B, H, q_len, D), device=Q.device, dtype=softmax_dtype)\n",
    "\n",
    "        q_idx = all_q_idx[q0:q1]\n",
    "\n",
    "        for k0 in range(0, T, block_k):\n",
    "            k1 = min(k0 + block_k, T)\n",
    "            Kblk = Kc[:, :, k0:k1, :]  # [B,H,k_len,D]\n",
    "            Vblk = Vc[:, :, k0:k1, :]  # [B,H,k_len,D]\n",
    "            k_idx = all_k_idx[k0:k1]\n",
    "\n",
    "            scores = torch.matmul(Qblk, Kblk.transpose(-2, -1)) * scale  # [B,H,q_len,k_len]\n",
    "\n",
    "            if causal:\n",
    "                causal_mask = (k_idx[None, :] > q_idx[:, None])  # [q_len,k_len]\n",
    "                scores = scores.masked_fill(causal_mask[None, None, :, :], float(\"-inf\"))\n",
    "\n",
    "            m_blk = scores.max(dim=-1).values\n",
    "            m_new = torch.maximum(m, m_blk)\n",
    "\n",
    "            exp_scores = torch.exp(scores - m_new[..., None])\n",
    "\n",
    "            alpha = torch.exp(m - m_new)\n",
    "            l = alpha * l + exp_scores.sum(dim=-1)\n",
    "            acc = alpha[..., None] * acc + torch.matmul(exp_scores, Vblk)\n",
    "\n",
    "            m = m_new\n",
    "\n",
    "        l_safe = torch.clamp(l, min=torch.finfo(softmax_dtype).tiny)\n",
    "        Oblk = acc / l_safe[..., None]\n",
    "        out[:, :, q0:q1, :] = Oblk.to(dtype=out.dtype)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979909d",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Correctness: showing the versions agree\n",
    "\n",
    "We now compare the streaming output to the naïve output across a range of shapes and dtypes, including sizes that are not multiples of the block sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeacd2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    B: int\n",
    "    H: int\n",
    "    T: int\n",
    "    D: int\n",
    "    causal: bool\n",
    "    dtype: torch.dtype\n",
    "\n",
    "def max_rel_err(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8) -> float:\n",
    "    num = (a - b).abs().max().item()\n",
    "    denom = (b.abs().max().item() + eps)\n",
    "    return num / denom\n",
    "\n",
    "def run_one_test(tc: TestCase, block_q=128, block_k=128):\n",
    "    B,H,T,D = tc.B, tc.H, tc.T, tc.D\n",
    "    Q = torch.randn(B,H,T,D, device=device, dtype=tc.dtype)\n",
    "    K = torch.randn(B,H,T,D, device=device, dtype=tc.dtype)\n",
    "    V = torch.randn(B,H,T,D, device=device, dtype=tc.dtype)\n",
    "\n",
    "    ref = attention_naive(Q.float(), K.float(), V.float(), causal=tc.causal).to(tc.dtype)\n",
    "    out = attention_streaming_online_softmax(Q, K, V, causal=tc.causal, block_q=block_q, block_k=block_k)\n",
    "\n",
    "    abs_err = (out - ref).abs().max().item()\n",
    "    rel_err = max_rel_err(out.float(), ref.float())\n",
    "    return abs_err, rel_err\n",
    "\n",
    "test_cases = [\n",
    "    TestCase(B=1,H=1,T=128,D=64,  causal=False, dtype=torch.float32),\n",
    "    TestCase(B=2,H=4,T=257,D=64,  causal=False, dtype=torch.float32),\n",
    "    TestCase(B=1,H=2,T=256,D=128, causal=True,  dtype=torch.float32),\n",
    "]\n",
    "\n",
    "if device == \"cuda\":\n",
    "    test_cases += [\n",
    "        TestCase(B=2,H=4,T=513,D=64,  causal=False, dtype=torch.float16),\n",
    "        TestCase(B=2,H=4,T=513,D=64,  causal=True,  dtype=torch.float16),\n",
    "    ]\n",
    "    try:\n",
    "        _ = torch.randn(1, device=\"cuda\", dtype=torch.bfloat16)\n",
    "        test_cases += [\n",
    "            TestCase(B=2,H=4,T=513,D=64,  causal=False, dtype=torch.bfloat16),\n",
    "            TestCase(B=2,H=4,T=513,D=64,  causal=True,  dtype=torch.bfloat16),\n",
    "        ]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "for tc in test_cases:\n",
    "    abs_err, rel_err = run_one_test(tc, block_q=128, block_k=128)\n",
    "    print(f\"B={tc.B} H={tc.H} T={tc.T} D={tc.D} causal={tc.causal} dtype={tc.dtype} | \"\n",
    "          f\"max_abs_err={abs_err:.3e} max_rel_err={rel_err:.3e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e552fd4",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Benchmarking memory and runtime\n",
    "\n",
    "The streaming version avoids allocating `[B,H,T,T]`. On GPU we can measure peak allocated memory during the call.\n",
    "\n",
    "In pure PyTorch, runtime may or may not improve because the streaming version does more Python-level looping and launches more kernels. The real win of FlashAttention comes when these steps are fused into a single CUDA kernel that keeps intermediate values on-chip. Still, the memory reduction shows up immediately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def benchmark(fn, warmup=5, iters=20):\n",
    "    for _ in range(warmup):\n",
    "        _ = fn()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        _ = fn()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0) / iters\n",
    "\n",
    "def peak_mem_bytes(fn):\n",
    "    if device != \"cuda\":\n",
    "        return None\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    _ = fn()\n",
    "    torch.cuda.synchronize()\n",
    "    return torch.cuda.max_memory_allocated()\n",
    "\n",
    "def run_bench_suite(B=2, H=8, D=64, dtype=None, causal=False, Ts=(512, 1024, 2048)):\n",
    "    if dtype is None:\n",
    "        dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "    rows = []\n",
    "    for T in Ts:\n",
    "        Q = torch.randn(B,H,T,D, device=device, dtype=dtype)\n",
    "        K = torch.randn(B,H,T,D, device=device, dtype=dtype)\n",
    "        V = torch.randn(B,H,T,D, device=device, dtype=dtype)\n",
    "\n",
    "        naive_fn = lambda: attention_naive(Q.float(), K.float(), V.float(), causal=causal).to(dtype)\n",
    "        stream_fn = lambda: attention_streaming_online_softmax(Q, K, V, causal=causal, block_q=128, block_k=128)\n",
    "\n",
    "        naive_time = benchmark(naive_fn)\n",
    "        stream_time = benchmark(stream_fn)\n",
    "\n",
    "        naive_mem = peak_mem_bytes(naive_fn)\n",
    "        stream_mem = peak_mem_bytes(stream_fn)\n",
    "\n",
    "        rows.append((T, naive_time, stream_time, naive_mem, stream_mem))\n",
    "    return rows\n",
    "\n",
    "rows = run_bench_suite(B=2, H=8, D=64, causal=True, Ts=(512, 1024, 2048))\n",
    "\n",
    "print(\"T | naive_ms | streaming_ms | naive_peak_mem_MB | streaming_peak_mem_MB\")\n",
    "for T, t_naive, t_stream, m_naive, m_stream in rows:\n",
    "    naive_ms = t_naive * 1000\n",
    "    stream_ms = t_stream * 1000\n",
    "    if m_naive is None:\n",
    "        print(f\"{T:4d} | {naive_ms:8.3f} | {stream_ms:11.3f} | (cpu) | (cpu)\")\n",
    "    else:\n",
    "        naive_mb = m_naive / (1024**2)\n",
    "        stream_mb = m_stream / (1024**2)\n",
    "        print(f\"{T:4d} | {naive_ms:8.3f} | {stream_ms:11.3f} | {naive_mb:17.1f} | {stream_mb:20.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d15e5",
   "metadata": {},
   "source": [
    "\n",
    "## 10. The bridge to CUDA (next post)\n",
    "\n",
    "What we implemented is the algorithmic core: tiled computation plus online softmax.\n",
    "\n",
    "The CUDA version will keep the same mathematical structure, but it will change *how* the work is scheduled:\n",
    "it will load tiles into on-chip memory, fuse operations, reduce global memory traffic, and compute outputs efficiently.\n",
    "\n",
    "A clean next step is to implement only the **forward** kernel first, validate it against the reference, and then move to backward once forward is solid.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
