{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102e9d95",
   "metadata": {},
   "source": [
    "# Deeply Learning 3: Mean Squared Error Loss Implementation from Scratch\n",
    "\n",
    "Deeply learning one concept at a time. In this post, we will implement mean squared error loss from scratch.\n",
    "\n",
    "## Introduction\n",
    "Mean squared error (MSE) loss is a commonly used loss function for regression problems. It measures the average squared difference between the predicted values and the ground truth values. The formula for MSE loss is:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the number of samples\n",
    "- $y_i$ is the true value for the $i$-th sample\n",
    "- $\\hat{y}_i$ is the predicted value for the $i$-th sample\n",
    "- $\\sum$ denotes the summation over all samples\n",
    "\n",
    "## Core Concept\n",
    "The core concept behind the mean squared error loss is to penalize the model for making predictions that are far from the true values. By squaring the differences, we ensure that larger errors are penalized more heavily than smaller ones. The MSE loss is smooth and differentiable, which makes it suitable for optimization using gradient descent.\n",
    "\n",
    "## Implementation from Scratch\n",
    "Let's implement the mean squared error loss from scratch as a Pytorch module. We then compare our implementation with the built-in `torch.nn.MSELoss` to ensure correctness. PyTorch also supports mean squared error loss with an optional `reduction` parameter that can be set to `'mean'`, `'sum'`, or `'none'` to specify how the loss should be aggregated across the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81509ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 1.0232\n",
      "PyTorch MSE Loss: 1.0232\n",
      "MSE Loss (sum reduction): 15.3479\n",
      "PyTorch MSE Loss (sum reduction): 15.3479\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Float\n",
    "\n",
    "class MSELossFromScratch(nn.Module):\n",
    "  def __init__(self, reduction: str = \"mean\"):\n",
    "    super().__init__()\n",
    "    self.reduction = reduction\n",
    "\n",
    "  def forward(\n",
    "    self,\n",
    "    y_pred: Float[torch.Tensor, \"batch_size output_dim\"],\n",
    "    y_true: Float[torch.Tensor, \"batch_size output_dim\"]\n",
    "    ) -> Float[torch.Tensor, \"\"]:\n",
    "    \"\"\"Computes the Mean Squared Error (MSE) loss between the predicted and true values.\"\"\"\n",
    "    # Ensure that the input tensors have the same shape\n",
    "    assert y_pred.shape == y_true.shape, \"Shape of y_pred and y_true must be the same\"\n",
    "    squared_diff = (y_pred - y_true) ** 2\n",
    "    if self.reduction == \"mean\":\n",
    "      mse_loss = squared_diff.mean()\n",
    "    elif self.reduction == \"sum\":\n",
    "      mse_loss = squared_diff.sum()\n",
    "    else:\n",
    "      mse_loss = squared_diff\n",
    "\n",
    "    return mse_loss\n",
    "\n",
    "# A simple test case to verify the implementation\n",
    "torch.manual_seed(123)\n",
    "y_pred = torch.randn(5, 3) # Predicted values (batch_size=5, output_dim=3)\n",
    "y_true = torch.randn(5, 3) # True values (batch_size=5, output_dim=3)\n",
    "mse_loss_fn = MSELossFromScratch()\n",
    "loss = mse_loss_fn(y_pred, y_true)\n",
    "print(f\"MSE Loss: {loss.item():.4f}\")\n",
    "\n",
    "torch_loss_fn = nn.MSELoss()\n",
    "torch_loss = torch_loss_fn(y_pred, y_true)\n",
    "print(f\"PyTorch MSE Loss: {torch_loss.item():.4f}\")\n",
    "\n",
    "mse_loss_fu_sum = MSELossFromScratch(reduction=\"sum\")\n",
    "loss_sum = mse_loss_fu_sum(y_pred, y_true)\n",
    "print(f\"MSE Loss (sum reduction): {loss_sum.item():.4f}\")\n",
    "\n",
    "torch_loss_sum = nn.MSELoss(reduction=\"sum\")\n",
    "torch_loss_sum_value = torch_loss_sum(y_pred, y_true)\n",
    "print(f\"PyTorch MSE Loss (sum reduction): {torch_loss_sum_value.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88dd0e9",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- Mean squared error loss measures the average squared difference between predicted and true values.\n",
    "- It penalizes larger errors more heavily, encouraging the model to make more accurate predictions.\n",
    "- The gradient of the loss with respect to the predicted values is used for backpropagation during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
