{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656cf431",
   "metadata": {},
   "source": [
    "# Deeply Learning 2: Cross Entropy Loss Implementation from Scratch\n",
    "Deeply learning one concept at a time. In this post, we will implement cross entropy loss from scratch.\n",
    "\n",
    "## Introduction\n",
    "Cross entropy loss is one of the most commonly used loss functions in classification problems. It measures the difference between two probability distributions: the true distribution (from the ground truth labels) and the predicted distribution (from the model's output).\n",
    "\n",
    "## The Core Idea\n",
    "The cross entropy loss for a single sample can be defined as:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{C} y_i \\log(p_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $C$ is the number of classes.\n",
    "- $y_i$ is the true label for class $i$ (1 if the sample belongs to class $i$, otherwise 0).\n",
    "- $p_i$ is the predicted probability for class $i$.\n",
    "\n",
    "Why log(p)? Idea is to capture how surprised we are when the model makes a wrong prediction. The logarithm here is used to penalize incorrect predictions more heavily. If the model predicts 99% probability for the correct class, resulting in low surprise, the loss will be low. However, if the model predicts a low probability for the correct class, the loss will be high, indicating a high level of surprise. This encourages that gradient flow will be stronger when the model is making incorrect predictions, which helps in faster learning.\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p_i} = -\\frac{y_i}{p_i}\n",
    "$$\n",
    "This gradient is used during backpropagation to update the model's parameters. \n",
    "\n",
    "## Implementation from Scratch\n",
    "Let's implement the cross entropy loss from scratch using just basic Pytorch operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0428d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "\n",
    "class CrossEntropyLossFromScratch(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "  \n",
    "  def forward(\n",
    "    self,\n",
    "    logits: Int[Tensor, \"batch_size num_classes\"],\n",
    "    targets: Int[Tensor, \"batch_size\"]\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "    batch_size, num_classes = logits.shape\n",
    "    assert targets.shape == (batch_size,)\n",
    "\n",
    "    max_logits, _ = torch.max(logits, dim=1, keepdim=True) #[batch_size, 1]\n",
    "    logits = logits - max_logits # stabize the logits #[batch_size, num_classes]\n",
    "    sum_exp_logits = torch.sum(torch.exp(logits), dim=1, keepdim=True) #[batch_size, 1]\n",
    "    log_probs = logits - torch.log(sum_exp_logits) #[batch_size, num_classes]\n",
    "    loss = -log_probs[torch.arange(batch_size), targets] #[batch_size]\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e46c1b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss: 1.4412\n",
      "PyTorch Cross Entropy Loss: 1.4412\n",
      "Losses are close: True\n"
     ]
    }
   ],
   "source": [
    "# a random logit tensor and target tensor for testing\n",
    "torch.manual_seed(0)\n",
    "logits = torch.randn(4, 3) # [batch_size, num_classes]\n",
    "targets = torch.tensor([0, 1, 2, 1]) # [batch_size]\n",
    "\n",
    "criterion = CrossEntropyLossFromScratch()\n",
    "loss = criterion(logits, targets)\n",
    "print(f\"Cross Entropy Loss: {loss.item():.4f}\")\n",
    "\n",
    "torch_loss = F.cross_entropy(logits, targets)\n",
    "print(f\"PyTorch Cross Entropy Loss: {torch_loss.item():.4f}\")\n",
    "\n",
    "print(f'Losses are close: {torch.isclose(loss, torch_loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84b204",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- Cross entropy loss captures the difference between true and predicted distributions.\n",
    "- It penalizes incorrect predictions more heavily, encouraging the model to learn faster.\n",
    "- The gradient of the loss with respect to the predicted probabilities is used for backpropagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
