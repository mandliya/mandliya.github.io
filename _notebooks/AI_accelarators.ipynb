{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abec4c9d-5522-4905-9120-6e967eaba556",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Intelligence (AI) Accelarators\n",
    "\n",
    "AI accelerators are specialized hardware designed to optimize the performance of AI workloads, including tasks like neural networks and machine learning. These devices handle data-intensive and computationally heavy processes more efficiently than standard hardware, and they can be deployed in data centers or on edge devices depending on the application.\n",
    "\n",
    "The three main types of AI processing hardware include:\n",
    "\n",
    "1.\t**Central Processing Unit (CPU)** â€“ General-purpose processors capable of handling AI tasks, especially with optimized libraries or high core counts. Common in both data centers and edge devices, though typically less efficient than specialized hardware for heavy AI workloads.\n",
    "2.\t**Graphics Processing Unit (GPU)** â€“ Parallel processors that excel at handling the matrix calculations essential for deep learning and high-performance AI tasks. Widely used in data centers for training large AI models, with some specialized, lower-power GPUs also available for edge devices.\n",
    "3.\t**Field-Programmable Gate Arrays (FPGA) / Application-Specific Integrated Circuits (ASIC)** â€“ Highly customizable (FPGA) or fixed-function (ASIC) chips designed for maximum efficiency in specific AI tasks. Frequently used in both data centers and edge environments, especially for low-latency applications like real-time video processing or autonomous driving.\n",
    "\n",
    "\n",
    "\n",
    "## How does AI Accelarators work\n",
    "\n",
    "AI Accelarators work in co-processor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3b58d-3087-4ddc-879b-bc35934ac1fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec337b1f-bbc4-4999-89bb-66244033a39e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2408196138.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    <img src=\"images/coprocessor_mode.png\" alt=\"Italian Trulli\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/coprocessor_mode.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "576dd7ad-2073-4261-a46d-0574a25f4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# from IPython.display import Image, display\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def mm(graph):\n",
    "#     graphbytes = graph.encode(\"utf8\")\n",
    "#     base64_bytes = base64.urlsafe_b64encode(graphbytes)\n",
    "#     base64_string = base64_bytes.decode(\"ascii\")\n",
    "#     display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "# mm(\"\"\"\n",
    "# flowchart TD\n",
    "#     style A fill:#b3cde3,stroke:#5b9bd5,stroke-width:2px\n",
    "#     style B fill:#ccebc5,stroke:#4daf4a,stroke-width:2px\n",
    "#     style C fill:#fbb4ae,stroke:#ff4d4d,stroke-width:2px\n",
    "#     style D fill:#fed9a6,stroke:#ff7f00,stroke-width:2px\n",
    "#     style E fill:#decbe4,stroke:#984ea3,stroke-width:2px\n",
    "#     style F fill:#d9d9d9,stroke:#737373,stroke-width:2px\n",
    "#     style G fill:#a6cee3,stroke:#1f78b4,stroke-width:2px\n",
    "#     style H fill:#b2df8a,stroke:#33a02c,stroke-width:2px\n",
    "#     style I fill:#ffcccc,stroke:#e31a1c,stroke-width:2px\n",
    "\n",
    "#     A([\"ðŸŽ¬ Start\"])\n",
    "#     B([\"ðŸ”„ Load source data to CPU\"])\n",
    "#     C([\"ðŸš€ Transfer data to accelerator unit\"])\n",
    "#     D([\"ðŸ’¾ Load data into accelerator memory\"])\n",
    "#     E([\"âš™ï¸ Send data for parallel processing\"])\n",
    "#     F([\"ðŸ“¥ Store result in global memory\"])\n",
    "#     G([\"â†©ï¸ Transfer data from accelerator unit to Host\"])\n",
    "#     H([\"ðŸ“ Write Result\"])\n",
    "#     I([\"ðŸ End\"])\n",
    "\n",
    "#     A --> B --> C --> G --> H --> I\n",
    "#     C --> D --> E --> F --> G\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d958e-ed4c-4315-b440-d1e092540722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
