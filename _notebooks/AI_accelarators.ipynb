{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abec4c9d-5522-4905-9120-6e967eaba556",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Intelligence (AI) Accelerators\n",
    "\n",
    "In the previous [post](https://mandliya.github.io/posts/LLM_inference_1/), we explored the intricacies of Large Language Model (LLM) inference, highlighting the challenges of latency, resource consumption, and scalability. To effectively address these challenges, it’s crucial to understand the role of AI hardware accelerators designed to enhance the performance of AI workloads, including LLM inference. Leveraging the right hardware can dramatically improve efficiency and cost-effectiveness when deploying LLMs at scale. This post delves into AI accelerators, examining their architecture, types, and their impact on optimizing LLM inference.\n",
    "\n",
    "## Understanding AI Accelerators\n",
    "\n",
    "AI accelerators are specialized hardware devices designed to accelerate artificial intelligence (AI) workloads, such as deep learning training and inference. They are optimized for the matrix calculations and parallel processing tasks that are essential for AI applications. AI accelerators are used in data centers, edge devices, and other computing environments to improve the performance, efficiency, and scalability of AI workloads. By reducing both energy consumption and computation time, AI accelerators allow us to scale AI applications that would otherwise be impractical on CPUs alone.\n",
    "\n",
    "## Types of AI Accelerators\n",
    "\n",
    "AI accelerators come in various forms, each optimized for different types of AI workloads. The three main types of AI accelerators are:\n",
    "\n",
    "1. **Graphics Processing Units (GPUs)**: GPUs were originally designed for rendering graphics in video games but have since become popular for AI workloads. Their architecture allows for simultaneous execution of multiple operations, and thus they excel at handling the matrix calculations and parallel processing tasks required for deep learning training and inference. GPUs are widely used in data centers for training large AI models and are also available in specialized, lower-power versions for edge devices. Examples of GPUs include NVIDIA Tesla, AMD Radeon, and Intel Xe.\n",
    "\n",
    "2. **Tensor Processing Units (TPUs)**: TPUs are custom-built AI accelerators developed by Google specifically for deep learning workloads. They are optimized for TensorFlow, Google's open-source machine learning framework, and are designed to accelerate both training and inference tasks. TPUs are available in Google Cloud Platform and are used in Google's data centers to power AI applications like Google Search, Google Photos, and Google Translate.\n",
    "\n",
    "3. **Field-Programmable Gate Arrays (FPGAs) / Application-Specific Integrated Circuits (ASICs)**: FPGAs and ASICs are highly customizable or fixed-function chips designed for maximum efficiency in specific AI tasks. They are used in both data centers and edge environments to accelerate AI workloads that require low latency and high throughput. FPGAs can be reprogrammed to adapt to different AI models, while ASICs are designed for specific tasks and offer maximum performance for those tasks. Examples of FPGAs include Intel Arria and Xilinx Alveo, while examples of ASICs include Google's Edge TPU and NVIDIA's Deep Learning Accelerator.\n",
    "\n",
    "## Key differences between CPUs and AI Accelerators\n",
    "\n",
    "CPUs and AI accelerators have different architectures and are optimized for different types of workloads. The table below highlights some key differences between CPUs and AI accelerators:\n",
    "\n",
    "| Feature | CPU | AI Accelerator |\n",
    "|---------|-----|----------------|\n",
    "| Architecture | General-purpose processor | Specialized hardware optimized for AI workloads |\n",
    "| Cores | Few (4-8) | Thousands |\n",
    "| Clock Speed | 2-4 GHz | 1-2 GHz |\n",
    "| Parallel Processing | Limited | High |\n",
    "| Memory | Large cache | High bandwidth memory |\n",
    "| Precision | High precision arithmetic | Low precision arithmetic |\n",
    "| Libraries | General-purpose | optimized for AI frameworks|\n",
    "| Energy Efficiency | Less efficient | More efficient |\n",
    "\n",
    "The diagram below shows the architecture of a CPU and a GPU, highlighting the differences in the number of cores and the parallel processing capabilities of the two types of processors:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/cpu_vs_gpu.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Note that in CPU there are fewer cores (4-8) and the design is optimized for low latency and high single-threaded performance. In contrast, GPUs have thousands of cores and are optimized for high throughput and parallel processing. This parallel processing capability allows GPUs to handle large-scale AI workloads efficiently.\n",
    "\n",
    "\n",
    "## Key Features of AI Accelerators & Impact on LLM Inference\n",
    "\n",
    "AI accelerators have several key features that make them well-suited for AI workloads e.g. LLM inference. These features include:\n",
    "\n",
    "- **Parallel Processing**: AI accelerators are designed to handle large-scale parallel workloads efficiently. They have multiple cores that can process data in parallel, allowing them to perform matrix calculations and other AI tasks quickly. This parallel processing capability is essential for LLM inference at scale, as the model needs to process large amounts of text data in parallel to achieve low latency and high throughput.\n",
    "\n",
    "- **High Bandwidth Memory**: AI accelerators have specialized memory that provides high bandwidth for fast data access. This allows them to load and process large datasets quickly, improving overall performance. Frequent data access is a key requirement for LLM inference, as the model needs to access the input text and the model parameters efficiently.\n",
    "\n",
    "- **Low Precision Arithmetic**: AI accelerators support low-precision arithmetic operations, such as 8-bit integer or 16-bit floating-point calculations. This reduces the memory footprint and energy consumption of AI workloads, making them more efficient. Low-precision arithmetic is beneficial for LLM inference, as it allows the model to process text data quickly without sacrificing accuracy.\n",
    "\n",
    "- **Optimized Libraries**: AI accelerators come with optimized libraries and frameworks that provide high-level APIs for common AI tasks. These libraries make it easy to develop and deploy AI models on the accelerators, reducing the time and effort required to optimize performance. These libraries often have integrated support for LLM models, making it easier to run LLM inference on AI accelerators. Examples of optimized libraries include TensorFlow, PyTorch, and cuDNN.\n",
    "\n",
    "- **Energy Efficiency**: AI accelerators are designed to be energy-efficient, consuming less power than general-purpose processors like CPUs. This makes them ideal for AI workloads that require large-scale processing and can benefit from the parallel processing capabilities of the accelerators. LLM inference at scale can be computationally intensive, and energy-efficient accelerators can help reduce the cost and environmental impact of running these workloads.\n",
    "\n",
    "- **Scalability**: AI accelerators are highly scalable, allowing them to handle large AI workloads efficiently. They can be deployed in clusters or data centers to scale up the processing power as needed. This scalability is essential for LLM inference, as the model needs to process large amounts of text data in real-time to provide low-latency responses. Deploying AI accelerators in a scalable architecture can help meet the performance requirements of LLM inference at scale.\n",
    "\n",
    "## Parallism in AI Accelerators\n",
    "\n",
    "There are four main types of parallelism in AI accelerators:\n",
    "\n",
    "### Data Parallelism\n",
    "\n",
    "Data parallelism involves splitting the input data into multiple batches and processing each batch in parallel. This is useful for AI workloads that involve large datasets, such as deep learning training. By processing the data in parallel, AI accelerators can reduce the time it takes to train large models and improve overall throughput. Data parallelism is commonly used in AI accelerators to accelerate tasks like matrix multiplications and convolutional operations.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/data_parallelism.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### Model Parallelism\n",
    "\n",
    "Model parallelism involves splitting the AI model into multiple parts and processing each part in parallel. This is useful for AI workloads that involve complex models with multiple layers and parameters. By processing the model in parallel, AI accelerators can reduce the time it takes to infer large models and improve overall performance. Model parallelism is commonly used in AI accelerators to accelerate tasks like LLM inference and image recognition.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/model_parallelism.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Pipeline Parallelism\n",
    "\n",
    "Pipeline parallelism involves splitting the AI workload into multiple stages and processing each stage in parallel. This is useful for AI workloads that involve sequential processing of data, such as natural language processing. By processing the workload in parallel, AI accelerators can reduce the time it takes to process large datasets and improve overall efficiency. Pipeline parallelism is commonly used in AI accelerators to accelerate tasks like speech recognition and machine translation.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/pipeline_parallelism.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "### Task Parallelism\n",
    "\n",
    "Task parallelism involves splitting the AI workload into multiple tasks and processing each task in parallel. This is useful for AI workloads that involve multiple independent tasks, such as autonomous driving. By processing the tasks in parallel, AI accelerators can reduce the time it takes to complete complex tasks and improve overall performance. Task parallelism is commonly used in AI accelerators to accelerate tasks like object detection and video analysis.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/task_parallelism.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Co-Processing Mode in AI Accelerators \n",
    "\n",
    "AI Accelerators often work in tandem with main CPU to offload the heavy computation tasks. The main CPU is responsible for the general purpose tasks and the AI Accelerators are responsible for the heavy computation tasks. This is usually called co-processing. Here is a simple diagram to show how the AI Accelerators work with the main CPU. Here is some brief nomenclature for co-processing:\n",
    "\n",
    "- **Host**: The main CPU. It is responsible for the main flow of the program. It orchestrates the task by loading the main data and handling input/output operations. In co-processing mode, the host initiates the process, transfers data to AI Accelerators, and receives the results. It handles all the non-computation logic and leaves the number crunching to the AI Accelerators.\n",
    "\n",
    "- **Device**: The AI Accelerators. They are responsible for the heavy computation tasks. After receiving data from the host, the accelerator loads it into its specialized memory and performs parallel processing optimized for AI workloads, such as matrix multiplications. Once it completes the processing, it stores the results and transfers them back to the host.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/coprocessor_mode.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "We will explore in more details how the AI Accelerators work in the following sections.\n",
    "\n",
    "## Task Vs Data Parallelism\n",
    "\n",
    "AI Accelerators can be designed to work in two main parallelism modes: Task Parallelism and Data Parallelism. In practice, many AI Accelerators support both task and data parallelism, allowing them to handle a wide range of AI workloads efficiently. The choice of parallelism mode depends on the specific requirements of the application and the workload being processed. Here is a brief overview of both:\n",
    "\n",
    "- **Task Parallelism**: In this mode, the AI Accelerators are designed to handle multiple tasks in parallel. This is useful when you have multiple independent tasks that can be executed simultaneously. For example, in a data center, you may have multiple users running different AI models at the same time. Task parallelism allows the AI Accelerators to handle these tasks concurrently, improving overall throughput. Task parallelism is also useful for edge devices that need to process multiple streams of data in real-time, such as autonomous vehicles or surveillance systems.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/task_parallelism.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "- **Data Parallelism**: In this mode, the AI Accelerators are designed to handle large datasets by splitting them into smaller chunks and processing them in parallel. This is useful when you have a single task that can be parallelized across multiple data points. For example, in deep learning training, you can split the training data across multiple AI Accelerators, each processing a subset of the data. Data parallelism allows you to scale up the training process and reduce the time it takes to train large models. SIMD (Single Instruction, Multiple Data) and SIMT (Single Instruction, Multiple Threads) are common techniques used to implement data parallelism in AI Accelerators. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/data_parallelism.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "## CPU vs GPU\n",
    "\n",
    "Let's compare the CPU and GPU in terms of their architecture and performance for AI workloads:\n",
    "\n",
    "- **CPU**: CPUs are general-purpose processors designed to handle a wide range of tasks, from running operating systems to executing complex algorithms. They are optimized for single-threaded performance and have a large cache to reduce memory latency. CPUs are well-suited for tasks that require high single-threaded performance, such as web browsing, office applications, and gaming. However, they are less efficient for parallel processing tasks like deep learning training, where GPUs excel. The CPU typically has a few cores (4-8) and a clock speed of 2-4 GHz. Their design is called \"latency oriented\" because they are optimized for low latency and high single-threaded performance.\n",
    "\n",
    "- **GPU**: GPUs are parallel processors designed to handle large-scale parallel workloads, such as deep learning training and scientific simulations. They have thousands of cores that can process data in parallel, making them ideal for tasks that require massive parallelism. GPUs are optimized for throughput and can process large amounts of data quickly. They are well-suited for AI workloads that involve matrix multiplications and other parallel operations. The GPU typically has thousands of cores and a clock speed of 1-2 GHz. Their design is called \"throughput oriented\" because they are optimized for high throughput and parallel processing.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/cpu_vs_gpu.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Say you need to multiply two matrices of size 1000x1000. The CPU would perform this operation sequentially, multiplying each element of the matrices one by one. The GPU, on the other hand, would perform this operation in parallel, with each core handling a different portion of the matrices. This parallel processing capability allows the GPU to perform matrix multiplications much faster than the CPU.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3b58d-3087-4ddc-879b-bc35934ac1fd",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec337b1f-bbc4-4999-89bb-66244033a39e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2408196138.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    <img src=\"images/coprocessor_mode.png\" alt=\"Italian Trulli\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/coprocessor_mode.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "576dd7ad-2073-4261-a46d-0574a25f4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# from IPython.display import Image, display\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def mm(graph):\n",
    "#     graphbytes = graph.encode(\"utf8\")\n",
    "#     base64_bytes = base64.urlsafe_b64encode(graphbytes)\n",
    "#     base64_string = base64_bytes.decode(\"ascii\")\n",
    "#     display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "# mm(\"\"\"\n",
    "# flowchart TD\n",
    "#     style A fill:#b3cde3,stroke:#5b9bd5,stroke-width:2px\n",
    "#     style B fill:#ccebc5,stroke:#4daf4a,stroke-width:2px\n",
    "#     style C fill:#fbb4ae,stroke:#ff4d4d,stroke-width:2px\n",
    "#     style D fill:#fed9a6,stroke:#ff7f00,stroke-width:2px\n",
    "#     style E fill:#decbe4,stroke:#984ea3,stroke-width:2px\n",
    "#     style F fill:#d9d9d9,stroke:#737373,stroke-width:2px\n",
    "#     style G fill:#a6cee3,stroke:#1f78b4,stroke-width:2px\n",
    "#     style H fill:#b2df8a,stroke:#33a02c,stroke-width:2px\n",
    "#     style I fill:#ffcccc,stroke:#e31a1c,stroke-width:2px\n",
    "\n",
    "#     A([\"🎬 Start\"])\n",
    "#     B([\"🔄 Load source data to CPU\"])\n",
    "#     C([\"🚀 Transfer data to accelerator unit\"])\n",
    "#     D([\"💾 Load data into accelerator memory\"])\n",
    "#     E([\"⚙️ Send data for parallel processing\"])\n",
    "#     F([\"📥 Store result in global memory\"])\n",
    "#     G([\"↩️ Transfer data from accelerator unit to Host\"])\n",
    "#     H([\"📝 Write Result\"])\n",
    "#     I([\"🏁 End\"])\n",
    "\n",
    "#     A --> B --> C --> G --> H --> I\n",
    "#     C --> D --> E --> F --> G\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d958e-ed4c-4315-b440-d1e092540722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
