{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cbe7ab",
   "metadata": {},
   "source": [
    "I am starting this series of blog posts to just jot down my learnings as I revisit the concepts of Deep Learning. This is just a fun way to keep learning, and committing a few posts per week to my blog. Just to start with, I'll implement a simple dropout layer from scratch and compare it with the PyTorch implementation. \n",
    "\n",
    "# Deeply Learning 1: Dropout Implementation from scratch\n",
    "\n",
    "Dropout is one of the most effective regularization technniques used in Deep Learning with very simple implementation. Let's understand how it works and then implement it from scratch.\n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "During training, we randomly drop (zero out) neurons of a layer with probability $p$ (dropout probability). The remaining neurons are scaled by the inverse of the dropout probability $1-p$ to keep the expected output the same as if the layer had not been dropped out.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\text{output}] &= \\mathbb{E}\\left[\\frac{x \\cdot \\text{mask}}{1-p}\\right] \\\\\n",
    "&= \\frac{x \\cdot \\mathbb{E}[\\text{mask}]}{1-p} \\\\\n",
    "&= \\frac{x \\cdot (1-p)}{1-p} \\\\\n",
    "&= x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Why does it work?\n",
    "\n",
    "#### Co-Adaptation of Features\n",
    "\n",
    "It forces the network to learn more robust and generalizable representations. Dropping units at random forces the network to avoid relying on any small set of neurons; representations become more distributed and redundant.\n",
    "\n",
    "#### Ensemble View\n",
    "With independent dropout masks, the network becomes an ensemble of all the sub-networks that can be formed by dropping out different neurons and averaging their outputs. This leads to a more robust and generalizable model.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "During training, we scale the activations of the remaining units by the inverse of the dropout probability $1-p$, while during inference, we do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9e7aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DropoutFromScratch(nn.Module):\n",
    "  def __init__(self, p: float = 0.5, inplace: bool = False):\n",
    "    super().__init__()\n",
    "    self.p = p\n",
    "    self.inplace = inplace\n",
    "\n",
    "  def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "    if not self.training or self.p == 0:\n",
    "      return input\n",
    "\n",
    "    keep_prob = 1 - self.p    \n",
    "    mask = (torch.rand_like(input) < keep_prob).float().to(input.device)\n",
    "\n",
    "\n",
    "    if self.inplace:\n",
    "      input.mul_(mask).div_(keep_prob)\n",
    "      return input\n",
    "    else:\n",
    "      return input * mask / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42da5feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats of output from DropoutFromScratch:\n",
      "Mean: 0.9994800090789795, Std: 1.0000048875808716\n",
      "Stats of output from PyTorch Dropout:\n",
      "Mean: 0.9958199858665466, Std: 0.9999962449073792\n"
     ]
    }
   ],
   "source": [
    "input = torch.ones(1000, 100)\n",
    "dropout = DropoutFromScratch(p=0.5)\n",
    "dropout.train()\n",
    "output = dropout(input)\n",
    "torch_dropout = nn.Dropout(p=0.5)\n",
    "torch_dropout.train()\n",
    "output_torch = torch_dropout(input)\n",
    "\n",
    "print('Stats of output from DropoutFromScratch:')\n",
    "print(f'Mean: {output.mean()}, Std: {output.std()}')\n",
    "print('Stats of output from PyTorch Dropout:')\n",
    "print(f'Mean: {output_torch.mean()}, Std: {output_torch.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa983f0",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- Dropout is a simple and effective regularization technique that helps prevent overfitting by randomly dropping out neurons during training.\n",
    "- It only affects the training process and does not change the inference process.\n",
    "- New random masks are generated for each forward pass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
