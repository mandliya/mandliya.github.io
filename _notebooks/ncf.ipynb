{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e06f4a8-ea82-483e-8b32-b5fe3c302957",
   "metadata": {},
   "source": [
    "# Research Paper Implementation : Neural Collaborative Filtering (NCF)\n",
    "\n",
    "In this post, we will implement the Neural Collaborative Filtering (NCF) model proposed in the research paper \"Neural Collaborative Filtering\" by Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu and Tat-Seng Chua. The paper was published in 2017 and can be accessed at the following link: [Neural Collaborative Filtering](https://arxiv.org/abs/1708.05031).\n",
    "\n",
    "## Contributions of the Paper\n",
    "\n",
    "The paper makes the following contributions:\n",
    "- Proposes a general framework for collaborative filtering using neural networks, enabling the learning of non-linear user-item interaction functions.\n",
    "- Combines the **linearity** of Generalized Matrix Factorization (GMF) with the **non-linearity** of Multi-Layer Perceptron (MLP) in a unified model called *NeuMF*.\n",
    "- Demonstrates the effectiveness of this hybrid approach on widely used recommendation datasets, outperforming traditional methods.\n",
    "\n",
    "## Background\n",
    "### Collaborative Filtering (CF) \n",
    "\n",
    "Collaborative Filtering (CF) is a technique used in recommender systems to predict a userâ€™s preferences based on patterns in historical user-item interactions. It can be broadly classified into:\n",
    "- **User-based CF** : Finds similar users to the target user and recommends items based on their preferences\n",
    "- **Item-based CF** : Finds items similar to those the user has interacted with and recommends them.\n",
    "\n",
    "### Matrix Factorization (MF)\n",
    "\n",
    "Matrix Factorization is a popular model-based CF approach that represents users and items as latent factors in a shared vector space. It predicts user-item interactions by computing the dot product of their corresponding latent factors. While effective, the dot-product operation limits the ability to model complex relationships between users and items.\n",
    "\n",
    "### Neural Collaborative Filtering (NCF)\n",
    "\n",
    "Previous work on collaborative filtering often used neural networks to encode auxiliary information (e.g., textual descriptions of items) while relying on traditional MF to model latent user-item interactions. The NCF model proposed in this paper replaces the rigid dot-product operation with a neural network, allowing it to learn complex, non-linear interaction functions. This approach is particularly suited for implicit feedback tasks, where the goal is to predict binary interactions (e.g., whether a user interacted with an item).\n",
    "\n",
    "## Implementation\n",
    "\n",
    "To reproduce and implement NCF, we will:\n",
    "\n",
    "1. Implement a baseline Matrix Factorization (MF) model.  \n",
    "2. Implement the Neural Collaborative Filtering (NCF) model.\n",
    "3. Train and evaluate the models on the MovieLens 100K dataset.\n",
    "4. Analyze the results and compare the performance of the models.\n",
    "\n",
    "## Metrics\n",
    "\n",
    "To compare the models, we will use the following metrics:\n",
    "\n",
    "- **Hit Ratio (HR)**: Measures whether the true positive (e.g., the ground-truth item) appears in the top-k recommendations for a user. A higher HR indicates better recall.\n",
    "- **Normalized Discounted Cumulative Gain (NDCG)**: Measures the ranking quality of the recommended items. It assigns higher scores to the items that are ranked higher in the list of recommendations.\n",
    "\n",
    "We will use leave-one-out evaluation, where one positive interaction per user is held out for testing, and the remaining interactions are used for training.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use MovieLens 100K dataset. The dataset can be downloaded from the following link: [MovieLens 100K](https://grouplens.org/datasets/movielens/100k/). The dataset has:\n",
    "- 100,000 ratings (1-5) from 943 users on 1682 movies.\n",
    "- Each user has rated at least 20 movies.\n",
    "- Some demographics for the user (age, gender, occupation, zip code) are also available.\n",
    "\n",
    "## Learning from Implicit Data\n",
    "\n",
    "In the context of implicit feedback, we treat any observed interaction (e.g., a user watching a movie) as a positive instance and unobserved interactions as negative instances. The goal is to predict whether a user will interact with an item or not. This has limitations as it assumes that the absence of interaction implies a negative preference, which may not always be true. However, it is a common approach in implicit feedback settings.\n",
    "Let's start by loading the dataset and preparing the data for training the models.\n",
    "\n",
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01010c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "      <td>L.A. Confidential (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "      <td>Heavyweights (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "      <td>Legends of the Fall (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "      <td>Jackie Brown (1997)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp                       title\n",
       "0      196      242       3  881250949                Kolya (1996)\n",
       "1      186      302       3  891717742    L.A. Confidential (1997)\n",
       "2       22      377       1  878887116         Heavyweights (1994)\n",
       "3      244       51       2  880606923  Legends of the Fall (1994)\n",
       "4      166      346       1  886397596         Jackie Brown (1997)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in the data (movielens), data is in u.data and u.item files\n",
    "path = 'data/movielens/'\n",
    "user_data = pd.read_csv(path + 'u.data', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "movie_data = pd.read_csv(path + 'u.item', sep='|', names=['item_id', 'title'], usecols=range(2), encoding='latin-1')\n",
    "\n",
    "# merge the data on item_id\n",
    "data = pd.merge(user_data, movie_data, on='item_id')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5816393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73fcde73",
   "metadata": {},
   "source": [
    "### Build the labels using Negative Sampling\n",
    "\n",
    "We will treat the user-item interactions as binary data, where any interaction is labeled as 1 (positive instance) and the unobserved interactions are labeled as 0 (negative instance). For every positive interaction, a fixed number of items that user hasn't interacted with randomly selected as negative. This is known as negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b16e05b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wt/ytfh33qx4fb1xtg_l7t55x4h0000gq/T/ipykernel_40088/43290574.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# create a new dataframe to store the negative samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mnegative_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mnegative_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmovie_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmovie_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# concatenate the positive and negative samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/blog/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# For each positive interaction (user, item) pair, sample 4 negative items\n",
    "\n",
    "# get the unique user and item ids\n",
    "users = data['user_id'].unique()\n",
    "items = data['item_id'].unique()\n",
    "\n",
    "# create a dictionary to store the negative samples\n",
    "negative_samples = {}\n",
    "for user in users:\n",
    "    # get the items that the user has interacted with\n",
    "    interacted_items = data[data['user_id'] == user]['item_id'].values\n",
    "    # get the items that the user has not interacted with\n",
    "    not_interacted_items = np.setdiff1d(items, interacted_items, assume_unique=True)\n",
    "    # sample 4 negative items\n",
    "    negative_samples[user] = np.random.choice(not_interacted_items, size=4, replace=False)\n",
    "\n",
    "# create a new dataframe to store the negative samples\n",
    "negative_data = pd.DataFrame(columns=['user_id', 'item_id', 'rating', 'timestamp', 'title'])\n",
    "for user in negative_samples.keys():\n",
    "    for item in negative_samples[user]:\n",
    "        negative_data = negative_data.append({'user_id': user, 'item_id': item, 'rating': 0, 'timestamp': 0, 'title': movie_data[movie_data['item_id'] == item]['title'].values[0]}, ignore_index=True)\n",
    "\n",
    "# concatenate the positive and negative samples\n",
    "data = pd.concat([data, negative_data])\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e51b1503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for MovieLens data, based on the specifications in the NCF paper.\n",
    "    Handles data loading, train-test split, and negative sampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, num_negatives=4, is_training=True, random_seed=42):\n",
    "        \"\"\"\n",
    "        Initialize the MovieLens dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        - path: Path to the MovieLens dataset (expects u.data and u.item files).\n",
    "        - num_negatives: Number of negative samples per positive sample.\n",
    "          - 4 for training.\n",
    "          - 99 for testing.\n",
    "        - is_training: Whether this dataset is for training or testing.\n",
    "        - random_seed: Seed for reproducibility.\n",
    "        \"\"\"\n",
    "        random.seed(random_seed)\n",
    "\n",
    "        # Load and preprocess data\n",
    "        self.data = self._load_data(path)\n",
    "        self.user_item_dict = self._build_interaction_dict(self.data)\n",
    "        self.all_items = set(self.data[\"item_id\"].unique())\n",
    "        self.num_negatives = num_negatives\n",
    "        self.is_training = is_training\n",
    "\n",
    "        # Build datasets\n",
    "        self.dataset = self._prepare_dataset()\n",
    "\n",
    "    def _load_data(self, path):\n",
    "        \"\"\"\n",
    "        Load and preprocess the MovieLens dataset.\n",
    "        \"\"\"\n",
    "        user_df = pd.read_csv(path + 'u.data', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "        movie_df = pd.read_csv(path + 'u.item', sep='|', names=['item_id', 'title'], usecols=range(2), encoding='latin-1')\n",
    "        data = pd.merge(user_df, movie_df, on='item_id')\n",
    "        data['interaction'] = 1  # Treat all ratings as positive interactions\n",
    "        return data\n",
    "\n",
    "    def _build_interaction_dict(self, data):\n",
    "        \"\"\"\n",
    "        Build a dictionary of user-item interactions.\n",
    "        \"\"\"\n",
    "        user_item_dict = defaultdict(set)\n",
    "        for _, row in data.iterrows():\n",
    "            user_item_dict[row[\"user_id\"]].add(row[\"item_id\"])\n",
    "        return user_item_dict\n",
    "\n",
    "    def _split_train_test(self):\n",
    "        \"\"\"\n",
    "        Split the data into training and testing sets. The latest interaction per user is assigned to the test set.\n",
    "        \"\"\"\n",
    "        train_items = {}\n",
    "        test_items = {}\n",
    "\n",
    "        for user, items in self.user_item_dict.items():\n",
    "            items = sorted(items)  # Ensure consistent ordering\n",
    "            test_item = items[-1]  # Latest interaction for testing\n",
    "            train_items[user] = set(items[:-1])  # Remaining interactions for training\n",
    "            test_items[user] = test_item  # Assign the test item\n",
    "\n",
    "        return train_items, test_items\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        Prepare the dataset for training or testing with appropriate negative sampling.\n",
    "        \"\"\"\n",
    "        dataset = []\n",
    "        train_items, test_items = self._split_train_test()\n",
    "\n",
    "        if self.is_training:\n",
    "            # Prepare training dataset\n",
    "            for user, positive_items in train_items.items():\n",
    "                for pos_item in positive_items:\n",
    "                    dataset.append((user, pos_item, 1))  # Positive sample\n",
    "                    # Add negative samples\n",
    "                    negative_items = random.sample(list(self.all_items - positive_items), self.num_negatives)\n",
    "                    dataset.extend((user, neg_item, 0) for neg_item in negative_items)\n",
    "        else:\n",
    "            # Prepare testing dataset\n",
    "            for user, test_item in test_items.items():\n",
    "                dataset.append((user, test_item, 1))  # Positive sample\n",
    "                # Add 99 negative samples for ranking\n",
    "                negative_items = random.sample(\n",
    "                    list(self.all_items - train_items[user] - {test_item}), self.num_negatives\n",
    "                )\n",
    "                dataset.extend((user, neg_item, 0) for neg_item in negative_items)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the size of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch a single sample from the dataset.\n",
    "        \"\"\"\n",
    "        return self.dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c73e855",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Example: Iterate through the training DataLoader\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 16\u001b[0m     users, items, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsers:\u001b[39m\u001b[38;5;124m\"\u001b[39m, users[:\u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItems:\u001b[39m\u001b[38;5;124m\"\u001b[39m, items[:\u001b[38;5;241m5\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set dataset path\n",
    "path = \"data/movielens/\"  # Replace with the correct path to your dataset\n",
    "\n",
    "# Create training dataset\n",
    "train_data = MovieLensDataset(path=path, num_negatives=4, is_training=True)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create testing dataset\n",
    "test_data = MovieLensDataset(path=path, num_negatives=99, is_training=False)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "# Example: Iterate through the training DataLoader\n",
    "for batch in train_loader:\n",
    "    users, items, labels = zip(*batch)\n",
    "    print(\"Users:\", users[:5])\n",
    "    print(\"Items:\", items[:5])\n",
    "    print(\"Labels:\", labels[:5])\n",
    "    break\n",
    "\n",
    "print(\"Train and test datasets are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076cfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
