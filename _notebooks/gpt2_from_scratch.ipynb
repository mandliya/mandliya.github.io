{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2yQN5FpKygj"
   },
   "source": [
    "# Building GPT-2 From Scratch:  Mechanistic Interpretability View\n",
    "\n",
    "In this post, we're going to build GPT-2 from the ground up fun and learning, implementing every component ourselves and understanding exactly how this remarkable architecture works. This is the first part of a two-part series. Here, we'll focus on understanding and implementing the architecture, loading pre-trained weights, and running inference. **In the next post, we'll train our implementation from scratch** and explore the training dynamics.\n",
    "\n",
    "We’ll build the model as a stack of simple, testable building blocks (LayerNorm → Embeddings → Attention → MLP → Transformer blocks → Unembedding). We’ll use the **“Anthropic / mechanistic interpretability” [view](https://transformer-circuits.pub/2021/framework/index.html)** of a transformer: keep attention heads explicit (separate $W_Q$, $W_K$, $W_V$, $W_O$ per head) because it makes later analysis much easier (patching, head attribution, interventions, etc.). Finally we’ll validate correctness by **loading GPT‑2 Small weights** from Neel Nanda’s reference [implementation](https://www.youtube.com/watch?v=bOYE6E8JrtU&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz) (via [EasyTransformer](https://github.com/redwoodresearch/Easy-Transformer)) and checking that our model predicts the same next tokens. \n",
    "\n",
    "\n",
    "Here’s a clean mental model for what we’re building (residual stream + attention + MLP as “writers” into the residual stream):\n",
    "\n",
    "![transformer architecture from Anthropic](../assets/img/building-gpt-2-from-scratch-mechanistic-interpretability-view/transformer_arch.png)\n",
    "\n",
    "\n",
    "In the a future post, we will actually train this model from scratch and explore various training optimizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAE4SclOND5W"
   },
   "source": [
    "# Setup\n",
    "\n",
    "We’ll install a couple of dependencies, import everything we need, and pick a device (`cuda` if available).  \n",
    "If you’re running this locally, make sure you have a recent PyTorch build and enough VRAM for GPT‑2 Small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJ4CslXyNDXv",
    "outputId": "faf11b3c-6d9f-40f0-cb9d-e721e4e7e0ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for easy_transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for PySvelte (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install what is missing\n",
    "%pip install -q git+https://github.com/neelnanda-io/Easy-Transformer.git@clean-transformer-demo\n",
    "# Install another version of node that makes PySvelte work way faster\n",
    "!curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash - >/dev/null;\n",
    "!sudo apt-get install -y nodejs -qq >/dev/null\n",
    "%pip install -q git+https://github.com/neelnanda-io/PySvelte.git\n",
    "%pip install -q fancy_einsum\n",
    "%pip install -q einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "anIaUmgpLvB8"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VT7t8-VnXKJI"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUGELj-lpiOR"
   },
   "source": [
    "## Reference model (for weight loading + sanity checks)\n",
    "\n",
    "As we build each component, it’s easy to accidentally get one small detail wrong (a transpose, a broadcast, a norm, a bias).\n",
    "\n",
    "To keep ourselves honest, we’ll use **Neel Nanda’s `EasyTransformer` GPT‑2 Small** as a reference:\n",
    "\n",
    "- We’ll **load its weights into our implementation** (same parameter names / shapes).\n",
    "- We’ll run a few prompts and verify that **the next-token predictions match**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lj0FNJJ7puxE",
    "outputId": "b224c47b-de2b-4a2f-8df2-b18960d0eed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    }
   ],
   "source": [
    "from easy_transformer import EasyTransformer #to compare our implementation with GPT-2\n",
    "from easy_transformer.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
    "\n",
    "reference_gpt2 = EasyTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=False,\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrMAMFM6qDJN",
    "outputId": "dae8d74e-0fdf-4d31-dcb0-e3ad8bc9e29b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " French\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I live in France, and I speak\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "log_probs = logits.log_softmax(dim=-1)\n",
    "next_token = logits[0, -1].argmax(dim=-1)\n",
    "print(reference_gpt2.tokenizer.decode(next_token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E57FMpUTgaI"
   },
   "source": [
    "## Config\n",
    "\n",
    "GPT‑2 has a very specific set of hyperparameters (embedding size, number of heads, MLP expansion, context length, etc.).\n",
    "\n",
    "We’ll define a `Config` dataclass that mirrors the GPT‑2 Small configuration and then reuse `cfg` everywhere.  \n",
    "Keeping these names consistent is also what makes weight loading painless later.\n",
    "\n",
    "We're implementing GPT-2 Small, which has:\n",
    "- **12 layers** (transformer blocks)\n",
    "- **12 attention heads** per layer\n",
    "- **768-dimensional** embeddings\n",
    "- **50,257 token** vocabulary\n",
    "- **1024 token** context window\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "veSoN7WJTrVr"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "  d_model: int = 768 #embedding size or residual stream size\n",
    "  n_layer: int = 12\n",
    "  n_head: int = 12 # number of attention head\n",
    "  d_mlp: int = 3072 # standard d_model x 4\n",
    "  n_ctx: int = 1024 # max sequence length or block size\n",
    "  layer_norm_eps: int = 1e-5\n",
    "  init_range: float = 0.02\n",
    "  debug: bool = True\n",
    "  d_vocab: int = 50257\n",
    "  d_head: int = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "P5mhYSziWuNt"
   },
   "outputs": [],
   "source": [
    "# some utils to print and test stuff\n",
    "\n",
    "def rand_float_test(cls, shape):\n",
    "  cfg = Config(debug=True)\n",
    "  layer = cls(cfg).to(device)\n",
    "  random_input = torch.randn(shape).to(device)\n",
    "  print(f\"{random_input.shape=}\")\n",
    "  output = layer(random_input)\n",
    "  print(f\"{output.shape=}\")\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "  cfg = Config(debug=True)\n",
    "  layer = cls(cfg).to(device)\n",
    "  random_input = torch.randint(100, 1000, shape).to(device)\n",
    "  print(f\"{random_input.shape=}\")\n",
    "  output = layer(random_input)\n",
    "  print(f\"{output.shape=}\")\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input_name, cache_dict=cache.cache_dict):\n",
    "  cfg = Config(debug=True)\n",
    "  layer = cls(cfg).to(device)\n",
    "  layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "  if isinstance(input_name, str):\n",
    "    reference_input = cache_dict[input_name]\n",
    "  else:\n",
    "    reference_input = input_name\n",
    "\n",
    "  reference_output = gpt2_layer(reference_input)\n",
    "  output = layer(reference_input)\n",
    "  print(f\"{reference_input.shape=}\")\n",
    "  print(f\"{reference_output.shape=}\")\n",
    "\n",
    "  comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "  print(f\"{comparison.sum() / comparison.numel():.2%} of the values match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqUzhGjhq3Js",
    "outputId": "a67aa294-bdcd-4bac-bbbb-aa3bf55665aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed: activation.shape=torch.Size([1, 9, 768])\n",
      "hook_pos_embed: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.0.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.0.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.0.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.0.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.0.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.0.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.0.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.0.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.0.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.0.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.0.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.0.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.0.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.0.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.0.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.0.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.0.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.1.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.1.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.1.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.1.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.1.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.1.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.1.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.1.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.1.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.1.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.1.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.1.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.1.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.1.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.1.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.1.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.1.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.2.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.2.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.2.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.2.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.2.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.2.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.2.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.2.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.2.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.2.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.2.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.2.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.2.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.2.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.2.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.2.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.2.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.3.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.3.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.3.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.3.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.3.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.3.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.3.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.3.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.3.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.3.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.3.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.3.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.3.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.3.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.3.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.3.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.3.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.4.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.4.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.4.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.4.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.4.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.4.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.4.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.4.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.4.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.4.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.4.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.4.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.4.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.4.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.4.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.4.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.4.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.5.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.5.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.5.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.5.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.5.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.5.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.5.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.5.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.5.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.5.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.5.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.5.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.5.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.5.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.5.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.5.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.5.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.6.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.6.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.6.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.6.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.6.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.6.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.6.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.6.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.6.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.6.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.6.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.6.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.6.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.6.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.6.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.6.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.6.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.7.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.7.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.7.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.7.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.7.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.7.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.7.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.7.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.7.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.7.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.7.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.7.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.7.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.7.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.7.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.7.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.7.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.8.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.8.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.8.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.8.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.8.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.8.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.8.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.8.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.8.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.8.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.8.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.8.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.8.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.8.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.8.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.8.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.8.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.9.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.9.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.9.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.9.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.9.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.9.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.9.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.9.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.9.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.9.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.9.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.9.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.9.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.9.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.9.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.9.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.9.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.10.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.10.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.10.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.10.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.10.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.10.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.10.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.10.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.10.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.10.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.10.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.10.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.10.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.10.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.10.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.10.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.10.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.11.hook_resid_pre: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.11.ln1.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.11.ln1.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.11.attn.hook_q: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.11.attn.hook_k: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.11.attn.hook_v: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.11.attn.hook_attn_scores: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.11.attn.hook_attn: activation.shape=torch.Size([1, 12, 9, 9])\n",
      "blocks.11.attn.hook_z: activation.shape=torch.Size([1, 9, 12, 64])\n",
      "blocks.11.hook_attn_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.11.hook_resid_mid: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.11.ln2.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "blocks.11.ln2.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.11.mlp.hook_pre: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.11.mlp.hook_post: activation.shape=torch.Size([1, 9, 3072])\n",
      "blocks.11.hook_mlp_out: activation.shape=torch.Size([1, 9, 768])\n",
      "blocks.11.hook_resid_post: activation.shape=torch.Size([1, 9, 768])\n",
      "ln_final.hook_scale: activation.shape=torch.Size([1, 9, 1])\n",
      "ln_final.hook_normalized: activation.shape=torch.Size([1, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.cache_dict.items():\n",
    "  print(f\"{activation_name}: {activation.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUHU19i1NfuR"
   },
   "source": [
    "## Building Block 1: LayerNorm\n",
    "\n",
    "LayerNorm is our first component. GPT-2 uses **pre-layer norm**, meaning we normalize *before* each attention and MLP sublayer (not after, as in the original Transformer paper).\n",
    "\n",
    "### Why LayerNorm Matters\n",
    "\n",
    "LayerNorm stabilizes training by normalizing activations to have mean 0 and variance 1 across the feature dimension. Unlike BatchNorm (which normalizes across examples), LayerNorm normalizes each example independently:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ and $\\sigma^2$ are computed across the feature dimension (d_model)\n",
    "- $\\gamma$ and $\\beta$ are learned scale and shift parameters\n",
    "- $\\epsilon$ prevents division by zero\n",
    "\n",
    "This is crucial for deep networks — without it, activations can explode or vanish as they flow through layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "kr2WEn_bNbY5"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, cfg: Config):\n",
    "    super().__init__()\n",
    "    self.cfg = cfg\n",
    "    self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "    self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # x.size = [batch, seq_len, d_model]\n",
    "    mean = einops.reduce(x, \"batch seq_len d_model -> batch seq_len 1\", \"mean\")\n",
    "    x_mean_diff = x - mean\n",
    "    var = einops.reduce(x_mean_diff.pow(2), \"batch seq_len d_model -> batch seq_len 1\", \"mean\")\n",
    "    std = torch.sqrt(var + self.cfg.layer_norm_eps)\n",
    "    normalized_x = x_mean_diff / std\n",
    "    normalized_x = normalized_x * self.w + self.b\n",
    "    return normalized_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGDacmbaW0cs",
    "outputId": "04baa639-46c3-4b89-9480-4b3727711aaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_input.shape=torch.Size([2, 4, 768])\n",
      "output.shape=torch.Size([2, 4, 768])\n",
      "reference_input.shape=torch.Size([1, 9, 768])\n",
      "reference_output.shape=torch.Size([1, 9, 768])\n",
      "100.00% of the values match\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(LayerNorm, [2, 4, 768])\n",
    "_ = load_gpt2_test(LayerNorm, reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7byNfj0XhRv"
   },
   "source": [
    "## Building Block 2: Token Embedding\n",
    "\n",
    "A language model starts with token IDs like:\n",
    "\n",
    "$$\n",
    "[15496,\\ 318,\\ 257,\\ 1332,\\ \\dots]\n",
    "$$\n",
    "\n",
    "We need to convert discrete tokens (integers from 0 to 50,256) into continuous vectors that the model can process.\n",
    "We map each token ID to a learned vector in $\\mathbb{R}^{d_{model}}$ using an embedding matrix:\n",
    "\n",
    "$$\n",
    "W_E \\in \\mathbb{R}^{d_{vocab} \\times d_{model}}\n",
    "$$\n",
    "\n",
    "Indexing into this table gives you token embeddings:\n",
    "\n",
    "$$\n",
    "x_{tok} = W_E[\\text{token\\_id}]\n",
    "$$\n",
    "\n",
    "Yes, PyTorch has `nn.Embedding`  but implementing it ourselves keeps the logic transparent and the parameter naming consistent with the reference model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "V2qRQvr0Xgn-"
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "  def __init__(self, cfg: Config):\n",
    "    super().__init__()\n",
    "    self.cfg = cfg\n",
    "    self.W_E = nn.Parameter(torch.empty(cfg.d_vocab, cfg.d_model))\n",
    "    nn.init.normal_(self.W_E, std=cfg.init_range)\n",
    "    # works like nn.Linear(cfg.d_vocab, cfg.d_model)\n",
    "\n",
    "  def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "    #[batch, seq_len] -> [batch, seq_len, d_model]\n",
    "    embed = self.W_E[tokens, :]\n",
    "    return embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEj7tSs3Wj2a",
    "outputId": "ef12652f-215e-43ae-9cf7-48c11d02d1c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_input.shape=torch.Size([2, 4])\n",
      "output.shape=torch.Size([2, 4, 768])\n",
      "reference_input.shape=torch.Size([1, 9])\n",
      "reference_output.shape=torch.Size([1, 9, 768])\n",
      "100.00% of the values match\n"
     ]
    }
   ],
   "source": [
    "rand_int_test(Embedding, [2, 4])\n",
    "load_gpt2_test(Embedding, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulUSlI2NaJwq"
   },
   "source": [
    "## Building Block 3: Positional Embedding\n",
    "\n",
    "Self-attention is *content-based* and *position agnostic*. It doesn’t inherently know whether a token is the 3rd token or the 300th token.\n",
    "\n",
    "GPT‑2 solves this with **learned positional embeddings**. For each position $p \\in [0, \\dots, n_{ctx}-1]$, we learn:\n",
    "\n",
    "$$\n",
    "W_{pos}[p] \\in \\mathbb{R}^{d_{model}}\n",
    "$$\n",
    "\n",
    "The model input at each position becomes:\n",
    "\n",
    "$$\n",
    "x = x_{tok} + x_{pos}\n",
    "$$\n",
    "\n",
    "This is the simplest approach (and it works surprisingly well). Later architectures add rotary embeddings (RoPE), ALiBi, etc., but GPT‑2 Small is learned absolute positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "2G81-E5pc565"
   },
   "outputs": [],
   "source": [
    "class PosEmbedding(nn.Module):\n",
    "  def __init__(self, cfg: Config):\n",
    "    super().__init__()\n",
    "    self.cfg = cfg\n",
    "    self.W_pos = nn.Parameter(torch.empty(cfg.n_ctx, cfg.d_model))\n",
    "    nn.init.normal_(self.W_pos, std=cfg.init_range)\n",
    "\n",
    "  def forward(self, tokens: torch.Tensor):\n",
    "    pos_embeds = self.W_pos[:tokens.size(1), :]\n",
    "    pos_embeds = einops.repeat(\n",
    "        pos_embeds,\n",
    "        \"seq_len d_model -> batch seq_len d_model\",\n",
    "        batch=tokens.size(0)\n",
    "    )\n",
    "    return pos_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5xYa-hpxDEX",
    "outputId": "09dfb42a-a1b6-4dd4-85f2-bac93b235ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_input.shape=torch.Size([3, 4])\n",
      "output.shape=torch.Size([3, 4, 768])\n",
      "reference_input.shape=torch.Size([1, 9])\n",
      "reference_output.shape=torch.Size([1, 9, 768])\n",
      "100.00% of the values match\n"
     ]
    }
   ],
   "source": [
    "_ = rand_int_test(PosEmbedding, [3, 4])\n",
    "_ = load_gpt2_test(PosEmbedding, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzNefl8hxJpX"
   },
   "source": [
    "## Building Block 4: Causal Self‑Attention\n",
    "\n",
    "This is where things get interesting. Attention is the mechanism that allows information to flow between token positions. It's what makes transformers so powerful.\n",
    "\n",
    "### The Attention Intuition\n",
    "\n",
    "Imagine you're reading the sentence: \"The trophy doesn't fit in the suitcase because **it** is too large.\" To understand what \"it\" refers to, you need to look back at previous words. Attention does exactly this — it lets each token \"attend to\" other tokens in the sequence. If you are less familiar with attention, I definitely recommend this Andrej Karpathy's [video](https://www.youtube.com/watch?v=kCc8FmEb1nY), I have also another post on visualizing attention [here](https://mandliya.github.io/posts/visualizing-attention-see-what-an-llm-sees/).\n",
    "\n",
    "### Multi-Head Attention Mechanics\n",
    "\n",
    "For each token, we:\n",
    "1. **Create queries, keys, and values**: Linear projections of the input\n",
    "2. **Compute attention scores**: Query·Key^T tells us \"how much to attend\"\n",
    "3. **Apply causal mask**: Prevent looking at future tokens (this makes it autoregressive)\n",
    "4. **Softmax and weighted sum**: Attention weights × Values gives us the output\n",
    "\n",
    "GPT-2 uses **multi-head attention** — 12 heads in parallel, each learning different patterns. Some heads might learn syntax, others semantics, others positional relationships.\n",
    "\n",
    "### Implementation Choice: Separate Q, K, V\n",
    "\n",
    "Our key implementation difference from standard libraries: we use separate weight matrices `W_Q`, `W_K`, `W_V` instead of a combined `c_attn`. This makes it trivial to:\n",
    "- Analyze what each head attends to\n",
    "- Intervene on specific attention patterns\n",
    "- Understand information routing\n",
    "\n",
    "The formula:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "oHEIe3DXxEgE"
   },
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "  def __init__(self, cfg: Config):\n",
    "    super().__init__()\n",
    "    assert cfg.d_model % cfg.n_head == 0\n",
    "    self.d_head = cfg.d_model // cfg.n_head\n",
    "    self.n_head = cfg.n_head\n",
    "    self.d_model = cfg.d_model\n",
    "    self.cfg = cfg\n",
    "    self.W_Q = nn.Parameter(torch.randn(self.n_head, self.d_model, self.d_head))\n",
    "    self.W_K = nn.Parameter(torch.randn(self.n_head, self.d_model, self.d_head))\n",
    "    self.W_V = nn.Parameter(torch.randn(self.n_head, self.d_model, self.d_head))\n",
    "    self.W_O = nn.Parameter(torch.randn(self.n_head, self.d_head, self.d_model))\n",
    "    self.b_Q = nn.Parameter(torch.zeros(self.n_head, self.d_head))\n",
    "    self.b_K = nn.Parameter(torch.zeros(self.n_head, self.d_head))\n",
    "    self.b_V = nn.Parameter(torch.zeros(self.n_head, self.d_head))\n",
    "    self.b_O = nn.Parameter(torch.zeros(self.d_model))\n",
    "    self.register_buffer('IGNORE', torch.tensor(-1e10, dtype=torch.float32, device=device))\n",
    "\n",
    "  def apply_causal_mask(self, attn: torch.Tensor) -> torch.Tensor:\n",
    "    #[b, n_head, query_pos, key_pos]\n",
    "    seq_len = attn.size(-2)\n",
    "    mask = torch.triu(\n",
    "        torch.ones(seq_len, seq_len, device=device),\n",
    "        diagonal=1\n",
    "    ).bool()\n",
    "    attn.masked_fill_(mask, self.IGNORE)\n",
    "    return attn\n",
    "\n",
    "  def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "    #[batch, seq_len d_model] -> [batch seq_len d_model]\n",
    "    q = einsum(\"batch query_pos d_model, n_head d_model d_head -> batch query_pos n_head d_head\",\n",
    "               input, self.W_Q) + self.b_Q\n",
    "    k = einsum(\"batch key_pos d_model, n_head d_model d_head -> batch key_pos n_head d_head\",\n",
    "               input, self.W_K) + self.b_K\n",
    "    v = einsum(\"batch key_pos d_model, n_head d_model d_head -> batch key_pos n_head d_head\",\n",
    "               input, self.W_V) + self.b_V\n",
    "\n",
    "    attn = einsum(\n",
    "        \"batch query_pos n_head d_head, batch key_pos n_head d_head -> batch n_head query_pos key_pos\",\n",
    "        q, k)\n",
    "    attn = attn / math.sqrt(self.d_head)\n",
    "    attn = self.apply_causal_mask(attn)\n",
    "    attn = attn.softmax(dim=-1)\n",
    "    z = einsum(\n",
    "        \"batch n_head query_pos key_pos, batch key_pos n_head d_head -> batch query_pos n_head d_head\",\n",
    "        attn, v\n",
    "    )\n",
    "\n",
    "    out = einsum(\n",
    "        \"batch query_pos n_head d_head, n_head d_head d_model -> batch query_pos d_model\",\n",
    "        z, self.W_O\n",
    "    ) + self.b_O\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CcMkd0AcYVX5",
    "outputId": "848fd7c9-df1b-42e7-8379-93432473a457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_input.shape=torch.Size([3, 4, 768])\n",
      "output.shape=torch.Size([3, 4, 768])\n",
      "reference_input.shape=torch.Size([1, 9, 768])\n",
      "reference_output.shape=torch.Size([1, 9, 768])\n",
      "100.00% of the values match\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(CausalAttention, [3, 4, 768])\n",
    "_ = load_gpt2_test(CausalAttention, reference_gpt2.blocks[0].attn, cache[\"blocks.0.ln1.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGNClXtMUfNg"
   },
   "source": [
    "## Building Block 5: MLP\n",
    "\n",
    "While attention *routes* information between positions, the MLP (Multi-Layer Perceptron) is where the actual *computation* happens. Think of it as: attention moves things around, MLP processes them.\n",
    "\n",
    "GPT‑2’s MLP is a 2-layer feed-forward network applied independently at each token position:\n",
    "\n",
    "$$\n",
    "\\text{MLP}(x) = W_{out}\\,\\text{GELU}(W_{in}x + b_{in}) + b_{out}\n",
    "$$\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The MLP is refreshingly simple:\n",
    "1. **Expand**: Linear layer projects from d_model (768) to 4×d_model (3072)\n",
    "2. **Activate**: GELU activation adds non-linearity\n",
    "3. **Contract**: Linear layer projects back down to d_model (768)\n",
    "\n",
    "### Why 4×?\n",
    "\n",
    "The 4× expansion is empirically determined. The intermediate layer needs to be larger than the input/output to increase model capacity. The current architecture runs:\n",
    "\n",
    "`768 → 3072 → 768`\n",
    "\n",
    "This MLP is applied identically and independently to each position, i.e. it's a position-wise feed-forward network.\n",
    "\n",
    "### GELU vs ReLU\n",
    "\n",
    "GPT-2 uses GELU (Gaussian Error Linear Unit) instead of ReLU. GELU is smoother and provides better gradients, especially important for the large-scale pre-training regime.\n",
    "\n",
    "Key details:\n",
    "- It expands dimensionality from $d_{model}$ to $d_{mlp}$ (typically 4×).\n",
    "- Uses **GELU** activation (smoother than ReLU; works well in transformers).\n",
    "- Runs per position (no mixing across tokens inside the MLP — that’s attention’s job).\n",
    "\n",
    "This is where a lot of the model’s capacity lives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "s4TgL4mSeBHV"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, cfg: Config):\n",
    "    super().__init__()\n",
    "    self.cfg = cfg\n",
    "    self.W_in = nn.Parameter(torch.randn(cfg.d_model, cfg.d_mlp))\n",
    "    self.b_in = nn.Parameter(torch.zeros(cfg.d_mlp))\n",
    "    self.gelu = nn.GELU()\n",
    "    self.W_out = nn.Parameter(torch.randn(cfg.d_mlp, cfg.d_model))\n",
    "    self.b_out = nn.Parameter(torch.randn(cfg.d_model))\n",
    "\n",
    "  def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "    z = einsum(\n",
    "        \"batch seq_len d_model, d_model d_mlp -> batch seq_len d_mlp\",\n",
    "        input, self.W_in\n",
    "    ) + self.b_in\n",
    "    z = self.gelu(z)\n",
    "    out = einsum(\n",
    "        \"batch seq_len d_mlp, d_mlp d_model -> batch seq_len d_model\",\n",
    "        z, self.W_out\n",
    "    ) + self.b_out\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qo6jTwEgfc6K",
    "outputId": "b09b36ec-df97-454e-9e3c-8453af889acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_input.shape=torch.Size([3, 4, 768])\n",
      "output.shape=torch.Size([3, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(CausalAttention, [3, 4, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmqKeTyNfjt3"
   },
   "source": [
    "## Building Block 6: Transformer Block\n",
    "\n",
    "Now we assemble the pieces into a complete transformer block. This is the fundamental repeating unit of GPT-2 - we'll stack 12 of these.\n",
    "\n",
    "### The Architecture Pattern\n",
    "\n",
    "Schematically (for residual stream $r$):\n",
    "\n",
    "$$\n",
    "r \\leftarrow r + \\text{Attn}(\\text{LN}(r))\n",
    "$$\n",
    "$$\n",
    "r \\leftarrow r + \\text{MLP}(\\text{LN}(r))\n",
    "$$\n",
    "\n",
    "### Why Residual Connections?\n",
    "\n",
    "The $r + \\dots$ is crucial. Residual connections (skip connections) allow gradients to flow directly through the network during training. Without them, deep networks would be nearly impossible to train. They also provide a conceptual benefit: each block can be viewed as applying a *refinement* or *correction* to the representation, rather than transforming it completely.\n",
    "\n",
    "### Pre-Norm vs Post-Norm\n",
    "\n",
    "We normalize *before* each sublayer (pre-norm). The original Transformer paper normalized after, but pre-norm has become standard because it stabilizes training for deeper models.\n",
    "The residual stream is the main state that flows through the network. Attention and MLP are “writers” that propose updates; the residual connection keeps information moving forward without being overwritten.\n",
    "\n",
    "Once we have a correct block, building the full model is just stacking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "IMZ5stPBfjYg"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, cfg: Config):\n",
    "    super().__init__()\n",
    "    self.cfg = cfg\n",
    "    self.ln1 = LayerNorm(cfg)\n",
    "    self.attn = CausalAttention(cfg)\n",
    "    self.ln2 = LayerNorm(cfg)\n",
    "    self.mlp = MLP(cfg)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    x = x + self.attn(self.ln1(x))\n",
    "    x = x + self.mlp(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nkMVD2uTfgak",
    "outputId": "d5bd6f64-f1ca-4a18-cfe4-e08fd01606e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_input.shape=torch.Size([3, 4, 768])\n",
      "output.shape=torch.Size([3, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(TransformerBlock, [3, 4, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUtjkSV1g23d"
   },
   "source": [
    "## Building Block 7: Unembedding\n",
    "\n",
    "After processing through 12 transformer blocks, we have rich representations for each token position. But we need to convert these back into predictions over the vocabulary.\n",
    "\n",
    "### The Unembedding Matrix\n",
    "\n",
    "The unembedding layer is a linear projection from $d_{model} (768)$ to $d_{vocab} (50,257)$. For each position, this gives us logits, a raw scores for each possible next token.\n",
    "$$\n",
    "W_U \\in \\mathbb{R}^{d_{model} \\times d_{vocab}}\n",
    "$$\n",
    "\n",
    "\n",
    "Logits for the next token are:\n",
    "\n",
    "$$\n",
    "\\text{logits} = r_{final} W_U\n",
    "$$\n",
    "\n",
    "\n",
    "### Weight Tying (Not used in this implementation)\n",
    "\n",
    "Some models tie the embedding and unembedding weights (they're transposes of each other). Original GPT-2 uses it but Neel's implementation doesn't. This gives more flexibility but costs more parameters. We don't do it in this implementation.\n",
    "\n",
    "At the end, we need to map the final residual stream back into vocabulary logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "Ffky_eHjgw4B"
   },
   "outputs": [],
   "source": [
    "class Unembedding(nn.Module):\n",
    "  def __init__(self, cfg: Config):\n",
    "    super().__init__()\n",
    "    self.cfg = cfg\n",
    "    self.W_U = nn.Parameter(torch.randn(cfg.d_model, cfg.d_vocab))\n",
    "    self.b_U = nn.Parameter(torch.zeros(cfg.d_vocab), requires_grad=False)\n",
    "\n",
    "  def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "    out = einsum(\n",
    "        \"batch seq_len d_model, d_model d_vocab -> batch seq_len d_vocab\",\n",
    "        input, self.W_U\n",
    "    ) + self.b_U\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvZBCu_YiALo"
   },
   "source": [
    "## Putting It All Together: The Complete GPT-2\n",
    "\n",
    "Now for the exciting part — we assemble all our building blocks into the complete transformer!\n",
    "\n",
    "### The Full Forward Pass\n",
    "\n",
    "Here's what happens when we feed in a sequence of tokens:\n",
    "\n",
    "1. **Embed**: Convert tokens to vectors and add positional encodings\n",
    "2. **Process**: Pass through 12 transformer blocks sequentially\n",
    "3. **Normalize**: Apply final LayerNorm\n",
    "4. **Unembed**: Project to vocabulary logits\n",
    "\n",
    "The beauty is that each block refines the representation, building up increasingly abstract and context-aware features.\n",
    "\n",
    "### Autoregressive Generation\n",
    "\n",
    "At inference time, GPT-2 generates one token at a time:\n",
    "- Feed in \"The cat\"\n",
    "- Get logits, sample the next token (say \"sat\")\n",
    "- Feed in \"The cat sat\"\n",
    "- Get next token (\"on\")\n",
    "- And so on...\n",
    "\n",
    "This is why the causal mask is so important, we can't peek at future tokens during generation because they don't exist yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "eKMiAAM0h_AJ"
   },
   "outputs": [],
   "source": [
    "class GPT2Transformer(nn.Module):\n",
    "  def __init__(self, cfg: Config):\n",
    "    super().__init__()\n",
    "    self.cfg = cfg\n",
    "    self.embed = Embedding(cfg)\n",
    "    self.pos_embed = PosEmbedding(cfg)\n",
    "    self.blocks = nn.ModuleList([\n",
    "        TransformerBlock(cfg)\n",
    "        for _ in range(cfg.n_layer)\n",
    "    ])\n",
    "    self.ln_final = LayerNorm(cfg)\n",
    "    self.unembed = Unembedding(cfg)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    emd = self.embed(x)\n",
    "    pos_emd = self.pos_embed(x)\n",
    "    residual = emd + pos_emd\n",
    "    for block in self.blocks:\n",
    "      residual = block(residual)\n",
    "\n",
    "    normalized_residual = self.ln_final(residual)\n",
    "    logits = self.unembed(normalized_residual)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhJg2uMpjWeM"
   },
   "source": [
    "## The Moment of Truth: Loading Pre-trained Weights\n",
    "\n",
    "Time to test if our implementation is correct! We'll load the pre-trained GPT-2 weights from our reference model into our custom implementation.\n",
    "\n",
    "If we've built everything correctly, our model should:\n",
    "1. Accept the weights without shape mismatches\n",
    "2. Generate coherent text\n",
    "3. Produce identical outputs to the reference model\n",
    "\n",
    "Let's instantiate our model and load the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "NEBsK9IxjSP0"
   },
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "my_model = GPT2Transformer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YAafEvBj0Sy",
    "outputId": "3544672f-56ca-4e12-ab05-18d2948708b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Transformer(\n",
       "  (embed): Embedding()\n",
       "  (pos_embed): PosEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): CausalAttention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (gelu): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm()\n",
       "  (unembed): Unembedding()\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "my_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "eimqvKcqkALm"
   },
   "outputs": [],
   "source": [
    "text = \"The 2021 Masters (officially the 2021 Betfred Masters) was a professional non-ranking snooker tournament that took place from 10 to 17 January 2021 at the Marshall Arena in Milton Keynes, England. It was the 47th staging of the Masters, which was first held in 1975, and the second of three Triple Crown events in the 2020–21 season. The top sixteen players from the snooker world rankings were invited to compete in a knockout tournament, organised by the World Professional Billiards and Snooker Association. It was played behind closed doors because of COVID-19 restrictions in the United Kingdom. The defending champion, Stuart Bingham, had defeated Ali Carter 10–8 in the 2020 Masters final. Bingham lost 6–5 to Yan Bingtao (pictured) in the semi-finals. Yan (one of three debutants at the event, alongside Thepchaiya Un-Nooh and Gary Wilson) met John Higgins in the final. Yan completed a 10–8 victory to win his \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140,
     "referenced_widgets": [
      "327e628db0234b93ae5eab635d99a5c0",
      "81dcf6c0f1ed43529fea3fa40fd0f697",
      "a5faf880e39247019adee9bdb50bbc95",
      "562663312639423c990e8ca5f6ad3956",
      "423f592cb92042a6933cbc51dc18bf71",
      "3188894501f34fc99325570a9594913e",
      "dcda5854e3ac474ebbab51e8b13210b5",
      "3f3b4e0119264eee99780f9592bde5cb",
      "0f1f806f2a0e475e865f7061f1df7977",
      "7b32a773cf134c49848a3e08005ac1a8",
      "c7ef7de544e94f77bb21c44e52fc2479"
     ]
    },
    "id": "3Y7duNFMklEE",
    "outputId": "511d77d8-3433-451d-b9b4-447ac5d97332"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327e628db0234b93ae5eab635d99a5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2021 Masters (officially the 2021 Betfred Masters) was a professional non-ranking snooker tournament that took place from 10 to 17 January 2021 at the Marshall Arena in Milton Keynes, England. It was the 47th staging of the Masters, which was first held in 1975, and the second of three Triple Crown events in the 2020–21 season. The top sixteen players from the snooker world rankings were invited to compete in a knockout tournament, organised by the World Professional Billiards and Snooker Association. It was played behind closed doors because of COVID-19 restrictions in the United Kingdom. The defending champion, Stuart Bingham, had defeated Ali Carter 10–8 in the 2020 Masters final. Bingham lost 6–5 to Yan Bingtao (pictured) in the semi-finals. Yan (one of three debutants at the event, alongside Thepchaiya Un-Nooh and Gary Wilson) met John Higgins in the final. Yan completed a 10–8 victory to win his vernacular title.\n",
      "\n",
      "\n",
      "The tournament was held in the Marshall Arena, Milton Keynes, England. The tournament was played in a closed-door, non-competitive manner. The tournament was held in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(100)):\n",
    "  test_tokens = reference_gpt2.to_tokens(text).to(device)\n",
    "  logits = my_model(test_tokens)\n",
    "  text += reference_gpt2.tokenizer.decode(logits[-1, -1].argmax())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140,
     "referenced_widgets": [
      "87e6da03acdf4c048951329c3e527217",
      "0952fa19b685425d8745cbd9bf7684d4",
      "ba4a3aa57b184ac4bb56444afcc9fe11",
      "be5387a2b54a45bf95035083c6140384",
      "dcb30681062443efa08fba815f5020dd",
      "e28ca821856d486eb9e06b5f750f25d1",
      "81371f53b8b04050839fb2ecf3b6fe0f",
      "adcfc4cdcbdd4413b7cb92c81c82da21",
      "c4e73ce634054c2a8cb9cba0c3baa046",
      "d0a1b149873d43e9ae71a6311b381d3e",
      "e165b5ab8207480e904563d18a038b03"
     ]
    },
    "id": "EaguRkquuW3M",
    "outputId": "06081dba-74c9-403c-cc21-127dacd86413"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e6da03acdf4c048951329c3e527217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2021 Masters (officially the 2021 Betfred Masters) was a professional non-ranking snooker tournament that took place from 10 to 17 January 2021 at the Marshall Arena in Milton Keynes, England. It was the 47th staging of the Masters, which was first held in 1975, and the second of three Triple Crown events in the 2020–21 season. The top sixteen players from the snooker world rankings were invited to compete in a knockout tournament, organised by the World Professional Billiards and Snooker Association. It was played behind closed doors because of COVID-19 restrictions in the United Kingdom. The defending champion, Stuart Bingham, had defeated Ali Carter 10–8 in the 2020 Masters final. Bingham lost 6–5 to Yan Bingtao (pictured) in the semi-finals. Yan (one of three debutants at the event, alongside Thepchaiya Un-Nooh and Gary Wilson) met John Higgins in the final. Yan completed a 10–8 victory to win his vernacular title.\n",
      "\n",
      "\n",
      "The tournament was held in the Marshall Arena, Milton Keynes, England. The tournament was played in a closed-door, non-competitive manner. The tournament was held in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner. The tournament was played in a closed-door, non-competitive manner\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(100)):\n",
    "  test_tokens = reference_gpt2.to_tokens(text).to(device)\n",
    "  logits = reference_gpt2(test_tokens)\n",
    "  text += reference_gpt2.tokenizer.decode(logits[-1, -1].argmax())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb97708",
   "metadata": {},
   "source": [
    "## What’s next: training from scratch\n",
    "\n",
    "In the next post, we’ll keep this exact model code (same module structure + parameter names) and add the missing pieces to **train GPT‑2 Small from scratch**:\n",
    "\n",
    "- dataset + tokenizer pipeline (and how to batch sequences efficiently)\n",
    "- causal language modeling loss (next-token prediction)\n",
    "- optimizer + learning rate schedule\n",
    "- training loop with logging, checkpointing, and evaluation\n",
    "- quick overfit tests and a small-scale run you can reproduce on a single GPU\n",
    "\n",
    "The nice part is: because we’ve already validated the forward pass against a known-good reference, any training issues will be about *optimization/data*, not mysterious architecture bugs.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0952fa19b685425d8745cbd9bf7684d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e28ca821856d486eb9e06b5f750f25d1",
      "placeholder": "​",
      "style": "IPY_MODEL_81371f53b8b04050839fb2ecf3b6fe0f",
      "value": "100%"
     }
    },
    "0f1f806f2a0e475e865f7061f1df7977": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3188894501f34fc99325570a9594913e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "327e628db0234b93ae5eab635d99a5c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81dcf6c0f1ed43529fea3fa40fd0f697",
       "IPY_MODEL_a5faf880e39247019adee9bdb50bbc95",
       "IPY_MODEL_562663312639423c990e8ca5f6ad3956"
      ],
      "layout": "IPY_MODEL_423f592cb92042a6933cbc51dc18bf71"
     }
    },
    "3f3b4e0119264eee99780f9592bde5cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "423f592cb92042a6933cbc51dc18bf71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "562663312639423c990e8ca5f6ad3956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b32a773cf134c49848a3e08005ac1a8",
      "placeholder": "​",
      "style": "IPY_MODEL_c7ef7de544e94f77bb21c44e52fc2479",
      "value": " 100/100 [00:03&lt;00:00, 30.73it/s]"
     }
    },
    "7b32a773cf134c49848a3e08005ac1a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81371f53b8b04050839fb2ecf3b6fe0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81dcf6c0f1ed43529fea3fa40fd0f697": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3188894501f34fc99325570a9594913e",
      "placeholder": "​",
      "style": "IPY_MODEL_dcda5854e3ac474ebbab51e8b13210b5",
      "value": "100%"
     }
    },
    "87e6da03acdf4c048951329c3e527217": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0952fa19b685425d8745cbd9bf7684d4",
       "IPY_MODEL_ba4a3aa57b184ac4bb56444afcc9fe11",
       "IPY_MODEL_be5387a2b54a45bf95035083c6140384"
      ],
      "layout": "IPY_MODEL_dcb30681062443efa08fba815f5020dd"
     }
    },
    "a5faf880e39247019adee9bdb50bbc95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f3b4e0119264eee99780f9592bde5cb",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0f1f806f2a0e475e865f7061f1df7977",
      "value": 100
     }
    },
    "adcfc4cdcbdd4413b7cb92c81c82da21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba4a3aa57b184ac4bb56444afcc9fe11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adcfc4cdcbdd4413b7cb92c81c82da21",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4e73ce634054c2a8cb9cba0c3baa046",
      "value": 100
     }
    },
    "be5387a2b54a45bf95035083c6140384": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0a1b149873d43e9ae71a6311b381d3e",
      "placeholder": "​",
      "style": "IPY_MODEL_e165b5ab8207480e904563d18a038b03",
      "value": " 100/100 [00:03&lt;00:00, 21.85it/s]"
     }
    },
    "c4e73ce634054c2a8cb9cba0c3baa046": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c7ef7de544e94f77bb21c44e52fc2479": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0a1b149873d43e9ae71a6311b381d3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcb30681062443efa08fba815f5020dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcda5854e3ac474ebbab51e8b13210b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e165b5ab8207480e904563d18a038b03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e28ca821856d486eb9e06b5f750f25d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
