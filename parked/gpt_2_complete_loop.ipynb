{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ada770966234be9a30bc14dd4bfb69c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_167492abf8104764b72aa13c53ffe7d7",
              "IPY_MODEL_665bb838f9274e80b9350c6ab48585ee",
              "IPY_MODEL_95eef6921a394806b0af5656b954f1b7"
            ],
            "layout": "IPY_MODEL_9fb7f93f4d3945c98c9ca4cef1f94687"
          }
        },
        "167492abf8104764b72aa13c53ffe7d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d3720f17136439fb2ca73ae7b1ca4eb",
            "placeholder": "​",
            "style": "IPY_MODEL_941b595afde9440f92d62c2ea9f67f8c",
            "value": "README.md: "
          }
        },
        "665bb838f9274e80b9350c6ab48585ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_111008e8ab1c4dfc849e4d4ae8075acd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_341a9e54bff843a6b466f438515c41e8",
            "value": 1
          }
        },
        "95eef6921a394806b0af5656b954f1b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b8f1b74cac24f55af2c225ccaf187f5",
            "placeholder": "​",
            "style": "IPY_MODEL_f30363e74aa74636afb31fb606e9d5c3",
            "value": " 1.06k/? [00:00&lt;00:00, 119kB/s]"
          }
        },
        "9fb7f93f4d3945c98c9ca4cef1f94687": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3720f17136439fb2ca73ae7b1ca4eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "941b595afde9440f92d62c2ea9f67f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "111008e8ab1c4dfc849e4d4ae8075acd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "341a9e54bff843a6b466f438515c41e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b8f1b74cac24f55af2c225ccaf187f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f30363e74aa74636afb31fb606e9d5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandliya/mandliya.github.io/blob/main/parked/gpt_2_complete_loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EwTaUzOqQKlS"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets jaxtyping tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from jaxtyping import Float, Int\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "import tiktoken\n",
        "from datasets import load_dataset\n",
        "from google.colab import userdata\n",
        "import pathlib\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "va8s9t4CQUnK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPT2Config:\n",
        "  n_layers: int = 12\n",
        "  d_model: int = 768\n",
        "  n_heads: int = 12\n",
        "  vocab_size: int = 50257\n",
        "  layer_norm_eps: float = 0.02\n",
        "  init_range: float = 0.02\n",
        "  dropout: float = 0.1\n",
        "  n_ctx: int = 1024\n",
        "  d_mlp: int = 4 * 768\n",
        "  weight_tying: bool = True"
      ],
      "metadata": {
        "id": "IRGRW3jhQfhv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Attention(nn.Module):\n",
        "  def __init__(self, cfg: GPT2Config):\n",
        "    super().__init__()\n",
        "    assert cfg.d_model % cfg.n_heads == 0, (\n",
        "        f\"{cfg.d_model} should be divisible by {cfg.n_heads}\"\n",
        "    )\n",
        "    self.cfg = cfg\n",
        "    self.c_attn = nn.Linear(cfg.d_model, 3 * cfg.d_model)\n",
        "    self.attn_dropout = nn.Dropout(cfg.dropout)\n",
        "    self.c_proj = nn.Linear(cfg.d_model, cfg.d_model)\n",
        "    self.resid_dropout = nn.Dropout(cfg.dropout)\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.tril(torch.ones(cfg.n_ctx, cfg.n_ctx))\n",
        "        .view(1, 1, cfg.n_ctx, cfg.n_ctx)\n",
        "    )\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      x: Float[Tensor, \"B T d_model\"]) -> Float[Tensor, \"B T d_model\"]:\n",
        "      B, T, d_model = x.shape\n",
        "      n_heads = self.cfg.n_heads\n",
        "      d_head = d_model // n_heads\n",
        "      qkv = self.c_attn(x) #[B, T, d_model * 3]\n",
        "      q, k, v = qkv.split(d_model, dim=2)\n",
        "      q = q.view(B, T, n_heads, d_head).transpose(1, 2) #[B nh T dh]\n",
        "      k = k.view(B, T, n_heads, d_head).transpose(1, 2)\n",
        "      v = v.view(B, T, n_heads, d_head).transpose(1, 2)\n",
        "\n",
        "      attn = q @ k.transpose(-2, -1) * (1.0 / math.sqrt(d_head))\n",
        "      attn = attn.masked_fill(\n",
        "          self.mask[:, :, :T, :T] == 0,\n",
        "          float('-inf')\n",
        "      )\n",
        "      attn = attn.softmax(dim=-1)\n",
        "      attn = self.attn_dropout(attn)\n",
        "      out = attn @ v #[B, nh, T, dh]\n",
        "      out = out.transpose(1, 2).contiguous().view(B, T, d_model)\n",
        "      return self.resid_dropout(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "G8KS-oCpQ1qk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2MLP(nn.Module):\n",
        "  def __init__(self, cfg: GPT2Config):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    self.c_fc = nn.Linear(cfg.d_model, cfg.d_mlp)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.c_proj = nn.Linear(cfg.d_mlp, cfg.d_model)\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "  def forward(self, x: Float[Tensor, \"B T d_model\"]) -> Float[Tensor, \"B T d_model\"]:\n",
        "    x = self.c_fc(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.c_proj(x)\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "o3LR0-yfS8Jw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Block(nn.Module):\n",
        "  def __init__(self, cfg: GPT2Config):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    self.ln_1 = nn.LayerNorm(cfg.d_model, eps=cfg.layer_norm_eps)\n",
        "    self.attn = GPT2Attention(cfg)\n",
        "    self.ln_2 = nn.LayerNorm(cfg.d_model, eps=cfg.layer_norm_eps)\n",
        "    self.mlp = GPT2MLP(cfg)\n",
        "\n",
        "  def forward(self, x: Float[Tensor, \"B T d_model\"]) -> Float[Tensor, \"B T d_model\"]:\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "d_7pqjn_TePj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Model(nn.Module):\n",
        "  def __init__(self, cfg: GPT2Config):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "        wte = nn.Embedding(cfg.vocab_size, cfg.d_model),\n",
        "        wpe = nn.Embedding(cfg.n_ctx, cfg.d_model),\n",
        "        embd_dropout = nn.Dropout(cfg.dropout),\n",
        "        h = nn.ModuleList([\n",
        "            GPT2Block(cfg) for _ in range(self.cfg.n_layers)\n",
        "        ]),\n",
        "        ln_f = nn.LayerNorm(cfg.d_model, eps=cfg.layer_norm_eps)\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size)\n",
        "    if cfg.weight_tying:\n",
        "      self.lm_head.weight = self.transformer.wte.weight\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "    for np, p in self.named_parameters():\n",
        "      if np.endswith('c_proj.weight'):\n",
        "        nn.init.normal_(p, mean=0.0, std=(cfg.init_range/math.sqrt(2 * cfg.n_layers)))\n",
        "\n",
        "  def _init_weights(self, module: nn.Module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      nn.init.normal_(module.weight, mean=0.0, std=self.cfg.init_range)\n",
        "      if module.bias is None:\n",
        "        nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      nn.init.normal_(module.weight, mean=0.0, std=self.cfg.init_range)\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      tokens: Int[Tensor, \"B T\"],\n",
        "      targets: Optional[Int[Tensor, \"B T\"]]\n",
        "    ) -> Tuple[Int[Tensor, \"B T vocab_size\"], Float[Tensor, \"\"]]:\n",
        "    B, T = tokens.shape\n",
        "    assert T <= self.cfg.n_ctx, (\n",
        "        f\"Sequence length {T} is longer than max sequence length: {self.cfg.n_ctx}\"\n",
        "    )\n",
        "    tok_emb = self.transformer.wte(tokens)\n",
        "    pos = torch.arange(0, T, dtype=torch.long, device=tokens.device)\n",
        "    pos_emb = self.transformer.wpe(pos)\n",
        "    residual = pos_emb + tok_emb\n",
        "    residual = self.transformer.embd_dropout(residual)\n",
        "    for block in self.transformer.h:\n",
        "      residual = block(residual)\n",
        "\n",
        "    residual = self.transformer.ln_f(residual)\n",
        "    if targets is not None:\n",
        "      logits = self.lm_head(residual) #[B T vocab_size]\n",
        "      loss = F.cross_entropy(\n",
        "          logits.view(-1, logits.size(-1)), #[B*T vocab_size]\n",
        "          targets.view(-1), #[B * T]\n",
        "          ignore_index=-1\n",
        "      )\n",
        "    else:\n",
        "      logits = self.lm_head(residual[:, [-1], :]) #[B 1 vocab_size]\n",
        "      loss = None\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(\n",
        "      self,\n",
        "      tokens: Int[Tensor, \"B T\"],\n",
        "      temperature: float = 1.0,\n",
        "      max_num_tokens: int = 256,\n",
        "      top_k: Optional[int] = None\n",
        "    ) -> Int[Tensor, \"B T+max_num_tokens\"]:\n",
        "    for _ in range(max_num_tokens):\n",
        "      tok_cond = (\n",
        "          tokens\n",
        "          if tokens.size(-1) <= self.cfg.n_ctx\n",
        "          else tokens[:, -self.cfg.n_ctx:]\n",
        "      )\n",
        "      logits, _ = self(tok_cond, targets=None) #[B T vocab_size]\n",
        "      logits = logits[:, -1, :] #[B vocab_size]\n",
        "      logits = logits / temperature\n",
        "      if top_k is not None:\n",
        "        k = min(k, self.cfg.vocab_size)\n",
        "        v, _ = torch.topk(logits, k) #[B k]\n",
        "        threshold = v[:, [-1]] #[B, 1]\n",
        "        logits.masked_fill_(logits < threshold, float('-inf'))\n",
        "      probs = logits.softmax(dim=-1) #[B vocab_size]\n",
        "      next_token = torch.multinomial(probs, num_samples=1) #[B, 1]\n",
        "      tokens = torch.cat((tokens, next_token), dim=1)\n",
        "    return tokens\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9cJCK-ElUFKD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, I am a large language model.\" * 500\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(text)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "print(f'Total tokens: {len(tokens)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL97nWzEXGfA",
        "outputId": "c1acae52-3afd-4c33-a747-0f63707c0100"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 4500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(tokens: Tensor, block_size: int, batch_size: int, device: str):\n",
        "  idx = torch.randint(0, len(tokens)-block_size, (batch_size,))\n",
        "  x = torch.stack([tokens[i: i+block_size] for i in idx])\n",
        "  y = torch.stack([tokens[i+1: i+block_size+1] for i in idx])\n",
        "  return x.to(device), y.to(device)"
      ],
      "metadata": {
        "id": "lmhBYy5MZI6x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "cfg = GPT2Config(dropout=0)\n",
        "model = GPT2Model(cfg)\n",
        "model = model.to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n",
        "steps = 200\n",
        "for step in range(steps):\n",
        "  x, y = get_batch(tokens, block_size=cfg.n_ctx, batch_size=2, device=device)\n",
        "  optimizer.zero_grad()\n",
        "  _, loss = model(x, targets=y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if step % 10 == 0:\n",
        "    print(f'{step=:4} | Loss: {loss.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys4jfsRhZpAr",
        "outputId": "9e86d393-8993-4457-876f-b3de6fa011b9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=   0 | Loss: 11.091410\n",
            "step=  10 | Loss: 5.922181\n",
            "step=  20 | Loss: 5.396425\n",
            "step=  30 | Loss: 5.022036\n",
            "step=  40 | Loss: 4.661715\n",
            "step=  50 | Loss: 4.316027\n",
            "step=  60 | Loss: 3.979648\n",
            "step=  70 | Loss: 3.658325\n",
            "step=  80 | Loss: 3.350863\n",
            "step=  90 | Loss: 3.031948\n",
            "step= 100 | Loss: 2.147627\n",
            "step= 110 | Loss: 1.343678\n",
            "step= 120 | Loss: 0.310863\n",
            "step= 130 | Loss: 0.093907\n",
            "step= 140 | Loss: 0.061922\n",
            "step= 150 | Loss: 0.048684\n",
            "step= 160 | Loss: 0.039142\n",
            "step= 170 | Loss: 0.033269\n",
            "step= 180 | Loss: 0.029778\n",
            "step= 190 | Loss: 0.025852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = userdata.get('HF_TOKEN')\n"
      ],
      "metadata": {
        "id": "sCtFAtykh0rN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DatasetConfig:\n",
        "  out_dir: str = './data'\n",
        "  write_batch_size: int = 128\n",
        "  hf_dataset:str = 'roneneldan/TinyStories'\n",
        "  max_examples: int = 600000\n",
        "  n_ctx: int = 1024"
      ],
      "metadata": {
        "id": "HMdRz5as5-8s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretokenize_and_save(config: DatasetConfig, split='train'):\n",
        "  dataset = load_dataset(\n",
        "      config.hf_dataset,\n",
        "      split=split,\n",
        "      streaming=True,\n",
        "  )\n",
        "  enc = tiktoken.get_encoding('gpt2')\n",
        "  os.makedirs(config.out_dir, exist_ok=True)\n",
        "  data_path = pathlib.Path(config.out_dir) / f'{split}.bin'\n",
        "  total_tokens = 0\n",
        "  with open(data_path, mode='wb') as f:\n",
        "    batch_tokens = []\n",
        "    for i, example in enumerate(tqdm(dataset)):\n",
        "      if i >= config.max_examples:\n",
        "        break\n",
        "      tokens = enc.encode_ordinary(example['text'])\n",
        "      batch_tokens.extend(tokens)\n",
        "      total_tokens += len(tokens)\n",
        "      if (i + 1) % config.write_batch_size == 0:\n",
        "        chunk = np.array(batch_tokens, dtype=np.uint16)\n",
        "        f.write(chunk.tobytes())\n",
        "        batch_tokens = []\n",
        "\n",
        "    if batch_tokens:\n",
        "      chunk = np.array(chunk, dtype=np.uint16)\n",
        "      f.write(chunk.tobytes())\n",
        "\n",
        "  print(f'Wrote {total_tokens=:,} to path: {str(data_path)}')\n"
      ],
      "metadata": {
        "id": "KS-Dh72w5-5m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = DatasetConfig()\n",
        "pretokenize_and_save(config)"
      ],
      "metadata": {
        "id": "TjXdgfEE5-23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "0ada770966234be9a30bc14dd4bfb69c",
            "167492abf8104764b72aa13c53ffe7d7",
            "665bb838f9274e80b9350c6ab48585ee",
            "95eef6921a394806b0af5656b954f1b7",
            "9fb7f93f4d3945c98c9ca4cef1f94687",
            "7d3720f17136439fb2ca73ae7b1ca4eb",
            "941b595afde9440f92d62c2ea9f67f8c",
            "111008e8ab1c4dfc849e4d4ae8075acd",
            "341a9e54bff843a6b466f438515c41e8",
            "8b8f1b74cac24f55af2c225ccaf187f5",
            "f30363e74aa74636afb31fb606e9d5c3"
          ]
        },
        "outputId": "b6ba95f9-f516-45ff-9f63-c3ed1528b4fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ada770966234be9a30bc14dd4bfb69c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "600000it [02:20, 4269.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote total_tokens=134,054,957 to path: data/train.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretokenize_and_save(config, split='validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTY23WOnAayj",
        "outputId": "a7b3e790-8f54-459d-a529-77ed6e649502"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "21990it [00:06, 3316.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote total_tokens=4,743,928 to path: data/validation.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenDataset(Dataset):\n",
        "  def __init__(self, config: DatasetConfig, split='train'):\n",
        "    super().__init__()\n",
        "    self.block_size = config.n_ctx\n",
        "    path = pathlib.Path(config.out_dir) / f'{split}.bin'\n",
        "    self.tokens = np.memmap(path, dtype=np.uint16, mode='r')\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens) // self.block_size\n",
        "\n",
        "  def __getitem__(self, index: int) -> Tuple[Tensor, Tensor]:\n",
        "    start = index * self.block_size\n",
        "    chunk = torch.from_numpy(\n",
        "        self.tokens[start: start+1+self.block_size].astype(np.int64)\n",
        "    )\n",
        "    x = chunk[:-1]\n",
        "    y = chunk[1:]\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "TeuMnj9j5-z1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TrainingConfig:\n",
        "  lr: float = 3e-5\n",
        "  log_steps: int = 1000\n",
        "  max_iters: int = 100\n",
        "  train_batch_size: int = 8\n",
        "  val_batch_size: int = 8\n",
        "  out_path: str = './out'\n"
      ],
      "metadata": {
        "id": "eB5HXAnKBCCd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    model: GPT2Model,\n",
        "    loader: DataLoader,\n",
        "    max_batches: int = 20) -> float:\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "    losses = []\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    with torch.no_grad():\n",
        "      for i, (x, y) in enumerate(tqdm(loader)):\n",
        "        if i >= max_batches:\n",
        "          break\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, loss = model(x, targets=y)\n",
        "        losses.append(loss.item())\n",
        "        print(f'Input: {enc.decode(x[0])}\\nOutput: {enc.decode(logits[len(x[0]):])}')\n",
        "    return sum(losses) / len(losses)"
      ],
      "metadata": {
        "id": "wizkFen9DX1I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "config = GPT2Config()\n",
        "model = GPT2Model(config).to(device)\n",
        "data_config = DatasetConfig()\n",
        "train_dataset = TokenDataset(data_config, split='train')\n",
        "val_dataset = TokenDataset(data_config, split='validation')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: GPT2Model,\n",
        "    train_dataset: TokenDataset,\n",
        "    valid_dataset: TokenDataset,\n",
        "    train_config: TrainingConfig):\n",
        "\n",
        "  train_dataloader = DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=train_config.train_batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=4,\n",
        "      pin_memory=True\n",
        "  )\n",
        "\n",
        "  val_dataloader = DataLoader(\n",
        "      val_dataset,\n",
        "      batch_size=train_config.val_batch_size,\n",
        "      shuffle=False,\n",
        "      pin_memory=False\n",
        "  )\n",
        "\n",
        "  best_val_loss = float('inf')\n",
        "  best_model_path = pathlib.Path(train_config.out_path) / f'best_model.pt'\n",
        "\n",
        "  for epoch in range(train_config.max_iters):\n",
        "    for step, (x, y) in enumerate(train_dataloader):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      _, loss = model(x, targets=y)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "\n",
        "      if step % train_config.log_steps == 0:\n",
        "        print(f'epoch {epoch:4} | step: {step:5d} | train_loss {loss.item():.4f}')\n",
        "\n",
        "\n",
        "    val_loss = evaluate(model, val_dataloader, max_batches=2000)\n",
        "    print(f'epoch: {epoch} | val loss: {val_loss:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      torch.save({\n",
        "          'epoch' : epoch,\n",
        "          'model' : model.state_dict(),\n",
        "          'optimizer': optimizer.state_dict(),\n",
        "          'val_loss' : val_loss,\n",
        "          'model_config': config,\n",
        "          'data_config': data_config,\n",
        "          'train_config': train_config\n",
        "      }, best_model_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jOvBN4V65-xR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_config = TrainingConfig()\n",
        "train(model, train_dataset=train_dataset, valid_dataset=val_dataset, train_config=train_config)"
      ],
      "metadata": {
        "id": "X8IOh8Lb5-uV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7e8657-71b4-4cba-a157-b1232cd76d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch    0 | step:     0 | train_loss 10.9824\n",
            "epoch    0 | step:  1000 | train_loss 10.9613\n",
            "epoch    0 | step:  2000 | train_loss 10.9674\n",
            "epoch    0 | step:  3000 | train_loss 10.9631\n",
            "epoch    0 | step:  4000 | train_loss 10.9626\n",
            "epoch    0 | step:  5000 | train_loss 10.9816\n",
            "epoch    0 | step:  6000 | train_loss 10.9872\n",
            "epoch    0 | step:  7000 | train_loss 11.0034\n",
            "epoch    0 | step:  8000 | train_loss 10.9915\n",
            "epoch    0 | step:  9000 | train_loss 10.9861\n",
            "epoch    0 | step: 10000 | train_loss 10.9900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DHCv9WzM5-rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZnYFmMmWGtld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8hKmNcxGtiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bcq47zzCGtf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJmXU9KfGtdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oweoi2UCGtah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WEaUwTIyGtXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4GXJAL5RGtVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aLV1NRnjGtSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ngEq_6vGtPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uth7imoIGtMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nGOjGaioGtI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c43zWWNMGtGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DatasetConfig:\n",
        "  out_dir: str = './out'\n",
        "  hf_dataset: str = 'roneneldan/TinyStories'\n",
        "  write_batch_size: int = 100\n",
        "  max_examples: int = 10000\n",
        "  n_ctx: int = 1024\n"
      ],
      "metadata": {
        "id": "YwOVmcK6afvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretokenize_and_save_dataset(\n",
        "    d_config: DatasetConfig,\n",
        "    split: str = 'train'\n",
        "  ) -> None:\n",
        "  dataset = load_dataset(\n",
        "      d_config.hf_dataset,\n",
        "      split=split,\n",
        "      streaming=True\n",
        "  )\n",
        "  os.makedirs(d_config.out_dir, exist_ok=True)\n",
        "  data_path = pathlib.Path(d_config.out_dir) / f'{split}.bin'\n",
        "  with open(data_path, mode='wb') as f:\n",
        "    batch_tokens = []\n",
        "    total_tokens = 0\n",
        "\n",
        "    for i, example in enumerate(tqdm(dataset)):\n",
        "      if i >= d_config.max_examples:\n",
        "        break\n",
        "      tokens = enc.encode_ordinary(example['text'])\n",
        "      tokens.append(enc.eot_token)\n",
        "      batch_tokens.extend(tokens)\n",
        "      total_tokens += len(tokens)\n",
        "      if (i+1) % d_config.write_batch_size:\n",
        "        chunk = np.array(batch_tokens, dtype=np.uint16)\n",
        "        f.write(chunk.tobytes())\n",
        "        batch_tokens = []\n",
        "\n",
        "    if batch_tokens:\n",
        "      chunk = np.array(batch_tokens, dtype=np.uint16)\n",
        "      f.write(chunk.tobytes())\n",
        "  print(f'Wrote {total_tokens=} to path: {str(data_path)}')"
      ],
      "metadata": {
        "id": "bzv30bbSlLoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_config = DatasetConfig()\n",
        "pretokenize_and_save_dataset(d_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld_t9556nYNm",
        "outputId": "c22a2720-4209-4dc2-8833-a139bd6baca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10000it [00:22, 446.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote total_tokens=2162078 to path: out/train.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretokenize_and_save_dataset(d_config, split='validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "2nHYteX_ARp_",
        "outputId": "3bb4deae-b71c-4af6-cecc-06bfee914f03"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pretokenize_and_save_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1748778094.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretokenize_and_save_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pretokenize_and_save_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TokenDataset(Dataset):\n",
        "  def __init__(self, config: DatasetConfig, split:str = 'train'):\n",
        "    super().__init__()\n",
        "    self.block_size = config.n_ctx\n",
        "    path = pathlib.Path(config.out_dir) / f'{split}.bin'\n",
        "    self.tokens = np.memmap(path, dtype=np.uint16, mode='r')\n",
        "    print(f'Total tokens: {len(self.tokens):,}')\n",
        "    print(f'Total chunks: {len(self):,}')\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens) // self.block_size\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    start = index * self.block_size\n",
        "    chunk = torch.from_numpy(\n",
        "        self.tokens[start: start + self.block_size + 1].astype(np.int64)\n",
        "    )\n",
        "    x = chunk[:-1]\n",
        "    y = chunk[1:]\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "zA2GS2T5nfhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TokenDataset(d_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4WVXz2qo1MU",
        "outputId": "f306da90-1b83-4cd7-c94a-77434e1f1c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 2,162,078\n",
            "Total chunks: 2,111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "cfg = GPT2Config(dropout=0)\n",
        "model = GPT2Model(cfg)\n",
        "model = model.to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n",
        "steps = 20000\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, pin_memory=True, )\n",
        "for step in range(steps):\n",
        "  x, y = next\n",
        "  optimizer.zero_grad()\n",
        "  _, loss = model(x, targets=y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if step % 10 == 0:\n",
        "    print(f'{step=:4} | Loss: {loss.item():.6f}')"
      ],
      "metadata": {
        "id": "_sl767BHpBWM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}